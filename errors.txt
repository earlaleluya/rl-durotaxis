   └─ Episode summary: 31 total nodes tracked, avg 4.4 nodes/step, range 2-6 nodes
Episode  603: R=-58.635 (Smooth=-67.44) | MB=48.0 | Steps=  7 | Success=False | Loss=36896.7472
/home/arlyan/miniconda3/envs/durotaxis/lib/python3.12/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/mnt/c/GitHub/rl-durotaxis/train.py", line 4182, in <module>
    main()
  File "/mnt/c/GitHub/rl-durotaxis/train.py", line 4178, in main
    trainer.train()
  File "/mnt/c/GitHub/rl-durotaxis/train.py", line 3295, in train
    losses = self.update_policy_minibatch(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/GitHub/rl-durotaxis/train.py", line 2902, in update_policy_minibatch
    return self.update_policy_with_value_clipping(states, actions, returns_dict, advantages_dict, old_log_probs, old_values_dict, episode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/GitHub/rl-durotaxis/train.py", line 2997, in update_policy_with_value_clipping
    hybrid_loss_dict = self.compute_hybrid_policy_loss(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/c/GitHub/rl-durotaxis/train.py", line 2623, in compute_hybrid_policy_loss
    if not torch.isfinite(total_policy_loss):
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Boolean value of Tensor with more than one value is ambiguous