{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362a4542",
   "metadata": {},
   "source": [
    "# Training of RL Agent in Durotaxis Simulation\n",
    "This file contains the history of training transactions of the proposed RL agent for the durotaxis simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5276e04a",
   "metadata": {},
   "source": [
    "## Workflow for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a6f58d",
   "metadata": {},
   "source": [
    "### 1. Initialize the durotaxis environment\n",
    "Here are the parameters for Durotaxis class:\n",
    "* `substrate_size` : Dimensions of the substrate environment (width, height) in pixels.\n",
    "* `substrate_type` : Type of substrate gradient ('linear', 'exponential').\n",
    "* `substrate_params` : Parameters for substrate generation: {'m': slope, 'b': intercept}.\n",
    "* `init_num_nodes` : Initial number of nodes when environment resets.\n",
    "* `max_critical_nodes` : Maximum allowed nodes before applying growth penalties ($N_c$).\n",
    "* `threshold_critical_nodes` : The critical threshold number of nodes such that episode terminates if exceeded (fail condition).\n",
    "* `max_steps` : Maximum steps per episode before timeout termination.\n",
    "* `embedding_dim` : Dimension of node embeddings for graph neural network processing.\n",
    "* `hidden_dim` : Hidden layer dimension for graph transformer networks.\n",
    "* `delta_time` : Time window ($\\Delta t$) for topology history comparison for node reduction rule which activates deletion if $I_{t+\\Delta t} \\leq \\epsilon$.\n",
    "* `delta_intensity` : Minimum intensity difference ($\\delta$) required for successful durotaxis spawning, such that $I(x',y') - I(x,y) \\geq \\delta$.\n",
    "    \n",
    "* `graph_rewards` : Graph-level reward components:\n",
    "    - `connectivity_penalty`: Penalty when nodes < 2 (loss of connectivity)\n",
    "    - `growth_penalty`: Penalty when nodes > max_nodes (excessive growth: $N > N_c$)\n",
    "    - `survival_reward`: Base reward for maintaining valid topology\n",
    "    - `action_reward`: Reward multiplier per action taken (encourages exploration)\n",
    "\n",
    "* `node_rewards` : Node-level reward components:\n",
    "    - `movement_reward`: Reward multiplier for rightward movement \n",
    "    - `intensity_penalty`: Penalty for nodes below average substrate intensity\n",
    "    - `intensity_bonus`: Bonus for nodes at/above average substrate intensity\n",
    "    - `substrate_reward`: Reward multiplier for substrate intensity values\n",
    "    \n",
    "* `edge_reward` : Edge direction rewards:\n",
    "    - `rightward_bonus`: Reward for edges pointing rightward (positive x-direction)\n",
    "    - `leftward_penalty`: Penalty for edges pointing leftward (negative x-direction)\n",
    "    \n",
    "* `spawn_rewards` : dict, Spawning behavior rewards:\n",
    "    - `span_success_reward`: Reward for successful durotaxis-based spawning\n",
    "    - `spawn_failure_penalty`: Penalty for spawning without sufficient intensity gradient\n",
    "\n",
    "* `delete_reward` : Deletion compliance rewards: \n",
    "    - `proper_deletion`: Reward for deleting nodes marked with to_delete flag\n",
    "    - `persistence_penalty`: Penalty for keeping nodes marked for deletion\n",
    "    \n",
    "* `position_rewards` : Positional behavior rewards:\n",
    "    - `boundary_bonus`: Bonus for nodes on topology boundary (frontier exploration)\n",
    "    - `left_edge_penalty`: Penalty for nodes near left substrate edge\n",
    "    - `edge_position_penalty`: Penalty for nodes near top/bottom substrate edges\n",
    "    \n",
    "* `termination_rewards` : Episode termination rewards:\n",
    "    - `success_reward`: Large reward for reaching rightmost substrate boundary\n",
    "    - `out_of_bounds_penalty`: Penalty for nodes moving outside substrate bounds\n",
    "    - `no_nodes_penalty`: Penalty for losing all nodes (topology collapse)\n",
    "    - `leftward_drift_penalty`: Penalty for consistent leftward centroid movement\n",
    "    - `timeout_penalty`: Small penalty for reaching maximum time steps\n",
    "    - `critical_nodes_penalty`: Penalty for exceeding critical node threshold\n",
    "    \n",
    "* `render_mode` : Visualization mode ('human' for real-time rendering, None for headless).\n",
    "* `flush_delay` : Delay between visualization updates (seconds) for rendering control.\n",
    "* `enable_visualization` : Enable/disable automatic topology visualization during episodes.\n",
    "* `model_path` : Base directory for saving models with automatic run organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from durotaxis_env import Durotaxis\n",
    "\n",
    "env = Durotaxis(\n",
    "        substrate_size=(200, 200),\n",
    "        substrate_type='linear',\n",
    "        substrate_params={'m': 0.01, 'b': 1.0},\n",
    "        init_num_nodes=1,\n",
    "        max_critical_nodes=50,\n",
    "        threshold_critical_nodes=200,\n",
    "        max_episodes=2,\n",
    "        max_steps=100,\n",
    "        embedding_dim=64,\n",
    "        hidden_dim=128,  \n",
    "        delta_time=3,\n",
    "        delta_intensity=2.50,\n",
    "        graph_rewards={\n",
    "            'connectivity_penalty': 10.0,  \n",
    "            'growth_penalty': 10.0,  \n",
    "            'survival_reward': 0.01,  \n",
    "            'action_reward': 0.005,  \n",
    "        },\n",
    "        node_rewards={\n",
    "            'movement_reward': 0.01,  \n",
    "            'intensity_penalty': 5.0,  \n",
    "            'intensity_bonus': 0.01,  \n",
    "            'substrate_reward': 0.05, \n",
    "        },\n",
    "        edge_reward={\n",
    "        'rightward_bonus': 0.1, \n",
    "        'leftward_penalty': 0.1},  \n",
    "        spawn_rewards={\n",
    "            'spawn_success_reward': 1.0, \n",
    "            'spawn_failure_penalty': 1.0,  \n",
    "        },\n",
    "        delete_reward={\n",
    "        'proper_deletion': 2.0, \n",
    "        'persistence_penalty': 2.0},  \n",
    "        position_rewards={\n",
    "            'boundary_bonus': 0.1,  \n",
    "            'left_edge_penalty': 0.2,  \n",
    "            'edge_position_penalty': 0.1, \n",
    "        },\n",
    "        termination_rewards={\n",
    "            'success_reward': 100.0, \n",
    "            'out_of_bounds_penalty': -30.0,  \n",
    "            'no_nodes_penalty': -30.0,  \n",
    "            'leftward_drift_penalty': -30.0,  \n",
    "            'timeout_penalty': -10.0,  \n",
    "            'critical_nodes_penalty': -25.0,  \n",
    "        },\n",
    "        enable_visualization=False,\n",
    "        model_path=\"./saved_models\",\n",
    "        save_per_episode=False  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abe826",
   "metadata": {},
   "source": [
    "### 2. Define a model and register it with the environment\n",
    "\n",
    "In the current code version, the model is represented with the policy agent using the customize Graph Transformer Policy, It can perform intelligent action selection. Thus, the creation of the model is internally performed. Although, you can rename the policy name for progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d72f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_algorithm_name(\"GraphTransformerPolicy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df891072",
   "metadata": {},
   "source": [
    "### 3. Train the Topology Policy Agent\n",
    "During training, the `enable_visualization` parameter is set to `False` so that it will only print the progress reports per episode, instead of providing the visualization of the agent within the substrate. This is to accelerate the training process, as visualization has periodic `flush_delay` to allow matplotlib to generate the plot without render issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8268b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444fa1f",
   "metadata": {},
   "source": [
    "### 4. Deploy the Trained Agent\n",
    "\n",
    "Run the following code in terminal:\n",
    "\n",
    "```bash\n",
    "conda activate durotaxis\n",
    "python durotaxis_sim.py\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "durotaxis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
