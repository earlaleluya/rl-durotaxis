# ============================================================================
# Durotaxis RL Configuration File
# ============================================================================
# Organized configuration for the refactored reward system
# Core components: Delete (Priority 1), Spawn (Priority 2), Distance (Priority 3)

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
environment:
  # Basic parameters
  substrate_size: [600, 400]
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology limits
  init_num_nodes: 70
  max_critical_nodes: 40  
  state_space_multiplier: 2.0  # Observation capacity: K = multiplier × max_critical_nodes
  threshold_critical_nodes: 400
  max_steps: 1000
  
  # Simulation parameters
  delta_time: 3
  consecutive_left_moves_limit: 30
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # ============================================================================
  # REWARD MODES (Special Modes for Ablation Studies)
  # ============================================================================
  # Normal mode (both disabled): Delete + Distance
  # Special modes: Enable ONE for ablation studies
  simple_delete_only_mode: false
  centroid_distance_only_mode: false
  include_termination_rewards: false
  
  # ============================================================================
  # REWARD WEIGHTS (Components: Delete, Distance, Termination)
  # ============================================================================
  # Environment-level weights applied when composing total reward
  # Makes priority explicit in the task reward, not just critic loss weights
  reward_weights:
    delete_weight: 1.0         
    distance_weight: 0.001       
    termination_weight: 0.01
  
  # ============================================================================
  # CORE REWARD COMPONENTS 
  # ============================================================================
  
  # Delete Reward (State Machine Design)
  # Node state transitions: UNMARKED → NOT_TO_DELETE → TO_DELETE → DELETED
  # Encourages growing beyond Nc (max_critical_nodes) and optimal deletion timing
  #
  # Key behaviors:
  #   1. Growth incentive: penalize UNMARKED nodes when N ≤ Nc
  #   2. Marking trigger: when N > Nc, mark nodes based on intensity vs average
  #   3. Timing constraint: TO_DELETE nodes must be deleted at t + delta_time
  #   4. State immutability: TO_DELETE nodes cannot be re-marked or reset
  #
  # Reward components:
  #   - Growth encouragement: penalty for N ≤ Nc to push beyond threshold
  #   - Unmarked penalty: force agent to trigger marking by growing
  #   - Exact deadline reward: bonus for deleting at correct timestep
  #   - Early/late penalty: discourage premature or delayed deletion
  #   - Wrong deletion penalty: prevent deleting NOT_TO_DELETE or UNMARKED nodes
  delete_reward:
    # Mode selection: 'homeostatic' (old) or 'state_machine' (new)
    mode: 'state_machine'
    
    # ========== STATE MACHINE MODE PARAMETERS ==========
    # Growth encouragement (push N > Nc)
    alpha: 1.0                      # Penalty weight when N ≤ Nc
    beta: 0.1                       # Bonus when N > Nc (marking activated)
    
    # Unmarked node penalty (force marking)
    gamma: 0.5                      # Penalty per UNMARKED node
    
    # Timing-based deletion rewards
    delta_pos: 5.0                  # Reward for deletion exactly at deadline
    delta_neg: 3.0                  # Penalty for early/late/missed deletion
    
    # Wrong deletion penalties
    zeta: 4.0                       # Penalty for deleting NOT_TO_DELETE node
    eta: 6.0                        # Penalty for deleting UNMARKED node
    
    # Timing tolerance (for floating point comparisons)
    epsilon_time: 0.000001               # 0.0 = strict exact, or small value like 1e-6
    
    # ========== OPTIMAL REWARD SHAPING (RECOMMENDED) ==========
    # Based on best practices: log scaling, progressive growth, smooth rewards
    
    # Growth incentive (small-N death spiral fix)
    kappa: 0.03                     # Progressive growth coefficient (when N ≤ Nc)
                                    # Range: [0.02, 0.05] - keeps growth attractive without overpowering
    
    # Marking incentives
    phi: 0.5                        # Continuous marking bonus coefficient (fraction_marked)
                                    # MUST be ≤ 0.5 to prevent dominance over timing rewards
                                    # Smooth reward for maintaining marked nodes
    
    # Penalty clipping (prevents unbounded negative spikes)
    deletion_penalty_clip: 30.0     # Max absolute penalty per step from wrong deletions
    deadline_penalty_clip: 30.0     # Max absolute penalty per step from missed deadlines
    normalize_penalties_by_n: true  # Normalize deletion/deadline penalties by (1+N)
    
    # Penalty scaling (prevents explosion and maintains gradients)
    use_log_scaling: true           # Use log(1 + n_unmarked) instead of linear scaling
    alpha_unmarked: 0.1             # Coefficient for U/(1+N) term (normalized unmarked)
    gamma_log: 0.5                  # Coefficient for log(1+U) term (log-scaled penalty)
    
    # Deadline reward smoothing
    use_smooth_deadline: true       # Use Gaussian instead of binary exact/wrong
    sigma_deadline: 0.5             # Gaussian width for deadline reward (steps)
                                    # σ=0.5 means reward drops to ~60% at ±1 step from deadline
    
    # Global normalization (auto-scales by system size)
    use_global_normalization: true  # Divide total reward by (1 + N)
                                    # Prevents large-N reward explosion automatically
    
    # ========== LEGACY PARAMETERS (for backward compatibility) ==========
    theta: 10.0                     # Old progressive growth scale (deprecated if kappa used)
    normalize_by_nc: false          # Old Nc-based normalization (deprecated if log scaling used)
    per_component_normalization: false  # Old per-component norm (deprecated if global norm used)
    
    # ========== HOMEOSTATIC MODE PARAMETERS (LEGACY) ==========
    # Homeostasis: reward for stable population in [N_min, N_max] band
    homeostasis_stable_reward: 0.3
    
    # Marking rewards (proportional to fraction of prev_N, capped to 1.0)
    marked_deleted_reward: 0.6      # Reward for deleting marked nodes
    marked_persist_penalty: -0.2    # Penalty for marked nodes that persist
    bad_delete_penalty: -0.8        # Penalty for deleting unmarked nodes
  
  # Distance Reward (Intensity-based centroid movement)
  # Measures centroid's substrate intensity increase (ΔI_centroid)
  # Goal: Reward agent for spawning on higher intensity substrate
  distance_mode:
    use_delta_distance: true
    distance_reward_scale: 5.0
    
    # Intensity-based distance reward
    target_delta_intensity: 2.5        # Target intensity increase per step
                                       # Reward = 0 at ΔI = target (threshold)
                                       # Reward > 0 only when ΔI > target
    
    # Substrate-aware normalization (bounds distance reward to [-1, 1])
    substrate_aware_scaling: true      # Use substrate gradient for scaling
    use_tanh_normalization: true       # Use tanh (true) or softsign (false)
    tanh_scale: 2.5                    # Scale factor: c = tanh_scale × target_delta_intensity
                                       # This ensures tanh(target_delta_intensity / c) = tanh(1) ≈ 0.76
                                       # For threshold at target: set c = target_delta_intensity (tanh(1)=0.76)
                                       # For zero at target: use shifted tanh or adjust calculation
    gradient_cap: null                 # Optional: cap gradient for exponential substrates (e.g., 10.0)
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: false
      shaping_coeff: 0.05             # Lowest among all components
      phi_distance_scale: 1.0         # Scale for distance-to-goal potential
  
  # Termination Reward (Priority 4: Sparse terminal signals)
  # Sparse binary rewards: +1 for success, -1 for failures
  # Applied only at episode termination
  termination_rewards:
    success_reward: 1.0                      # Node reaches rightmost area
    critical_nodes_penalty: -1.0             # Too many nodes (> threshold)
    no_nodes_penalty: -1.0                   # Graph collapses to 0 nodes
    out_of_bounds_penalty: -1.0              # Node escapes substrate bounds
    leftward_drift_penalty: -1.0             # Centroid drifts left consecutively
    timeout_penalty: 0.0                     # Max steps reached (neutral)
  
  # ============================================================================
  # OBSERVATION SELECTION (Deletion-Aware)
  # ============================================================================
  # Prioritizes nodes important for deletion decisions:
  # - Rightward nodes (x-coordinate): spatial progress
  # - High-intensity nodes: candidates for marking (low intensity = marked)
  # - Representationally important nodes: high embedding norm
  observation_selection:
    method: topk_x
    w_x: 0.6           # Rightward bias (durotaxis direction)
    w_intensity: 0.3   # Intensity-based selection (marking criterion)
    w_norm: 0.1        # Representational importance

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
encoder:
  out_dim: 128
  num_layers: 4

actor_critic:
  hidden_dim: 128
  continuous_dim: 5
  dropout_rate: 0.2
  pretrained_weights: 'imagenet'
  
  # Backbone configuration
  backbone:
    input_adapter: 1ch_conv
    freeze_mode: all
    backbone_lr: 1.0e-4
    head_lr: 3.0e-4
  
  # Action parameter bounds
  action_parameter_bounds:
    delete_ratio: [0.0, 0.7]
    gamma: [0.5, 5.0]
    alpha: [0.5, 3.0]
    noise: [0.05, 0.10]
    theta: [-0.3491, 0.3491]  # -20 to 20 deg
  
  # Value function components
  value_components:
    - 'total_reward'
    - 'delete_reward'
    - 'distance_reward'
    - 'termination_reward'
  
  # Simplicial Embedding
  simplicial_embedding:
    enabled: true
    num_groups: 16
    temperature: 1.0

# ============================================================================
# TRAINING ALGORITHM
# ============================================================================
algorithm:
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  clip_epsilon: 0.2
  value_loss_coeff: 0.25
  max_grad_norm: 0.5
  target_kl: 0.03

# ============================================================================
# TRAINER CONFIGURATION
# ============================================================================
trainer:
  substrate_type: random
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.0001
  save_dir: "./training_results"
  
  # Resume training
  resume_training:
    enabled: false
    checkpoint_path: ""
    resume_from_best: false
    reset_optimizer: true
    reset_episode_count: true
  
  # Model selection
  model_selection:
    primary_metric: success_rate
    primary_weight: 1000.0
    secondary_metric: progress
    secondary_weight: 100.0
    tertiary_metric: return_mean
    tertiary_weight: 1.0
    window_episodes: 50
    min_improvement: 0.01
    min_episode_for_best_save: 20
  
  # Learning rate schedule
  warmup_updates_for_best: 5
  lr_warmup_updates: 200
  lr_total_updates: 10000
  
  # Batch training
  rollout_collection_mode: "steps"
  rollout_steps: 2048
  rollout_batch_size: 10
  update_epochs: 4
  minibatch_size: 64
  
  # Value clipping
  enable_value_clipping: true
  value_clip_epsilon: 0.2
  use_relative_value_clip: true 
  
  # Return normalization (prevents value loss explosion)
  normalize_returns: true
  return_scale: 10.0
  
  # Logging
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null
  
  # Detailed logging
  detailed_logging:
    enable_detailed_logging: true
    save_detailed_logs_to_json: true
    single_file_mode: true
    log_node_positions: true
    log_spawn_parameters: true
    log_persistent_ids: true
    log_connectivity: false
    log_substrate_values: false
    compress_logs: false
    max_log_file_size_mb: 50
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]           # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]            # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]     # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]       # Base value range for exponential substrate
  
  # Optimization
  entropy_bonus_coeff: 0.1
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Component weights (for multi-head value function)
  # Normalized to sum to 1.0 for balanced critic head learning
  component_weights:
    total_reward: 0.4
    delete_reward: 0.4
    distance_reward: 0.1
    termination_reward: 0.1
  
  # Policy loss weights
  policy_loss_weights:
    continuous_weight: 1.0
    entropy_weight: 0.01
  
  # Gradient scaling
  gradient_scaling:
    enable_adaptive_scaling: true
    gradient_norm_target: 1.0
    scaling_momentum: 0.9
    min_scaling_factor: 0.1
    max_scaling_factor: 10.0
    warmup_steps: 100
  
  # Adaptive scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
  
  # Reward normalization
  reward_normalization:
    enable_per_episode_norm: true
    enable_cross_episode_scaling: true
    min_episode_length: 3
    normalization_method: 'zscore'
  
  # Advantage weighting
  advantage_weighting:
    enable_learnable_weights: true
    enable_attention_weighting: true
    weight_learning_rate: 0.01
    weight_regularization: 0.001
  
  # Entropy regularization
  entropy_regularization:
    enable_adaptive_entropy: true
    entropy_coeff_start: 0.5
    entropy_coeff_end: 0.1
    entropy_decay_episodes: 500
    continuous_entropy_weight: 1.0
    min_entropy_threshold: 0.5

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================
system:
  device: 'auto'
  num_workers: 1
  seed: null

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  tensorboard: false
  wandb: false
  save_model_every: null
  save_best_model: true
  verbose: true
