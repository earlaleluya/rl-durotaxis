# ============================================================================
# Durotaxis RL Configuration File
# ============================================================================
# Organized configuration for the refactored reward system
# Core components: Delete (Priority 1), Spawn (Priority 2), Distance (Priority 3)

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
environment:
  # Basic parameters
  substrate_size: [600, 400]
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology limits
  init_num_nodes: 70
  max_critical_nodes: 40  
  state_space_multiplier: 2.0  # Observation capacity: K = multiplier × max_critical_nodes
  threshold_critical_nodes: 500
  max_steps: 1000
  
  # Simulation parameters
  delta_time: 3
  consecutive_left_moves_limit: 30
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # ============================================================================
  # REWARD MODES (Special Modes for Ablation Studies)
  # ============================================================================
  # Normal mode (both disabled): Delete + Distance
  # Special modes: Enable ONE for ablation studies
  simple_delete_only_mode: false
  centroid_distance_only_mode: false
  include_termination_rewards: false
  
  # ============================================================================
  # REWARD WEIGHTS (Components: Delete, Distance, Termination)
  # ============================================================================
  # Environment-level weights applied when composing total reward
  # Makes priority explicit in the task reward, not just critic loss weights
  reward_weights:
    delete_weight: 1.0         
    distance_weight: 1.0 #  0.001       
    termination_weight: 0.01
  
  # ============================================================================
  # CORE REWARD COMPONENTS 
  # ============================================================================
  
  # Delete Reward (State Machine Design)
  # Node state transitions: UNMARKED → NOT_TO_DELETE → TO_DELETE → DELETED
  # Encourages growing beyond Nc (max_critical_nodes) and optimal deletion timing
  #
  # Key behaviors:
  #   1. Growth incentive: penalize UNMARKED nodes when N ≤ Nc
  #   2. Marking trigger: when N > Nc, mark nodes based on intensity vs average
  #   3. Timing constraint: TO_DELETE nodes must be deleted at t + delta_time
  #   4. State immutability: TO_DELETE nodes cannot be re-marked or reset
  #
  # Reward components:
  #   - Growth encouragement: penalty for N ≤ Nc to push beyond threshold
  #   - Unmarked penalty: force agent to trigger marking by growing
  #   - Exact deadline reward: bonus for deleting at correct timestep
  #   - Early/late penalty: discourage premature or delayed deletion
  #   - Wrong deletion penalty: prevent deleting NOT_TO_DELETE or UNMARKED nodes
  delete_reward:
    # Mode selection: 'homeostatic' (old) or 'state_machine' (new)
    mode: 'state_machine'
    
    # ========== STATE MACHINE MODE PARAMETERS ==========
    # Growth encouragement (push N > Nc)
    alpha: 1.0                      # Penalty weight when N ≤ Nc
    beta: 0.1                       # Bonus when N > Nc (marking activated)
    
    # Unmarked node penalty (force marking)
    gamma: 0.5                      # Penalty per UNMARKED node
    
    # ========== GAUSSIAN SOFT-DEADLINE TIMING REWARD ==========
    # Three-stage learning curriculum:
    #   Stage 1: Delete near deadline (± wide tolerance) - early training
    #   Stage 2: Improve precision gradually - middle training  
    #   Stage 3: Delete exactly at deadline - late training
    # No harsh penalties early on - smooth gradient for learning
    
    # Gaussian timing parameters
    sigma: 4.0                      # Softness of deadline timing (in timesteps)
                                    # σ=4.0 means reward drops to ~60% at ±4 steps from deadline
                                    # Larger σ = more forgiving early learning
    tolerance_window: 10            # Wide tolerance window (steps)
                                    # If |t - deadline| ≤ W: positive reward (scaled by Gaussian)
                                    # If |t - deadline| > W: small penalty
    k1: 0.5                         # Max timing bonus (at exact deadline)
    k2: 0.5                         # Penalty for deletion outside tolerance window
    
    # Wrong deletion penalties
    zeta: 0.5                       # Penalty for deleting NOT_TO_DELETE node
    eta: 0.5                        # Penalty for deleting UNMARKED node
    missed_penalty: 0.5             # Penalty for missed deadline (TO_DELETE node past deadline)
    
    # ========== GROWTH AND MARKING REWARDS ==========
    # Smooth shaping signals for learning progression
    
    # Growth reward (Gaussian centered at Nc - encourages N ≈ Nc)
    beta1: 0.5                      # Growth reward coefficient
                                    # R_growth = beta1 * exp(-[(N-Nc)²/(2σ²)])
                                    # Maximum at N=Nc, decreases on both sides
    growth_sigma: 20.0              # Gaussian width for growth reward
                                    # Larger σ = more tolerant of deviations from Nc
                                    # σ=20 means reward drops to ~60% at N=Nc±20
    
    # Marking reward (bonus when marking activates)
    beta2: 0.5                      # Marking event reward coefficient
                                    # R_marking = beta2 * (TD + NTD) / N when N > Nc
                                    # Explicit reward for triggering marking phase
    
    # Hysteresis for marking logic (prevents oscillation)
    hysteresis_epsilon: 0.05        # Margin around avg_intensity for stable marking
                                    # intensity < avg - ε → TO_DELETE
                                    # intensity > avg + ε → NOT_TO_DELETE
                                    # Prevents state flipping when intensity ≈ avg
    
    # ========== PENALTY NORMALIZATION ==========
    # Prevents magnitude explosions with large N
    
    # Unmarked penalty (normalized by Nc, not N)
    gamma: 0.5                      # Unmarked penalty coefficient
                                    # R_unmarked = -gamma * (U / Nc)
                                    # Range: [-gamma, 0] regardless of N
    
    # Global normalization method
    normalize_by_nc: true           # Use single tanh normalization at end
                                    # Raw components summed → tanh(sum / T)
                                    # NO per-component normalization (preserves gradients)
    
    # Temperature scaling for tanh (prevents early gradient compression)
    reward_temp: 2.0                # Scale R_raw before tanh: tanh(R_raw / T)
                                    # T=1.0 → standard tanh (compresses at |R|>3)
                                    # T=2.0 → preserves differences in [-3,+3] range
                                    # T=3.0 → even more forgiving
    
    # ========== ADVANCED STABILITY FEATURES ==========
    # Prevent dominance and instability issues
    
    # Adaptive timing (scale sigma and tolerance with delta_t)
    adaptive_timing: true           # If true: sigma = delta_t/2, tolerance = delta_t/3
                                    # Ensures learning difficulty constant regardless of deadline length
                                    # If false: use fixed sigma and tolerance_window values above
    
    # Deletion component clipping (prevents dominance)
    clip_deletion: 1.0              # Clip R_deletion to [-clip_del, +clip_del]
                                    # Prevents deletion reward from dominating when many nodes deleted
                                    # Recommended: 1.0 (same scale as other components)
    
    # Deadline component clipping (prevents unbounded penalties)
    clip_deadline: 0.5              # Clip R_deadline to [-clip_deadline, +clip_deadline]
                                    # Prevents missed-deadline penalties from overwhelming other components
                                    # Recommended: 0.5 (smaller than deletion since it's purely penalty)
    
    # UNMARKED age threshold (prevents permanent dead zone)
    unmarked_age_threshold: 10      # Force classification after this many steps in UNMARKED
                                    # Prevents nodes from staying UNMARKED forever in hysteresis dead zone
                                    # Lower = more aggressive forcing, Higher = more patient
    
    # ========== COMPONENT WEIGHTS ==========
    # Balance between different reward signals (all components normalized to [-1,+1])
    # Equal weights = equal learning importance
    weight_growth: 1.0              # Weight for growth reward
    weight_unmarked: 1.0            # Weight for unmarked penalty
    weight_marking: 1.0             # Weight for marking reward
    weight_deletion: 1.0            # Weight for deletion timing reward
    weight_deadline: 1.0            # Weight for missed deadline penalty
    # Marking rewards (proportional to fraction of prev_N, capped to 1.0)
    marked_deleted_reward: 0.6      # Reward for deleting marked nodes
    marked_persist_penalty: -0.2    # Penalty for marked nodes that persist
    bad_delete_penalty: -0.8        # Penalty for deleting unmarked nodes
  
  # Distance Reward (Intensity-based centroid movement)
  # Measures centroid's substrate intensity increase (ΔI_centroid)
  # Goal: Reward agent for spawning on higher intensity substrate
  distance_mode:
    use_delta_distance: true
    distance_reward_scale: 5.0
    
    # Intensity-based distance reward
    target_delta_intensity: 5.0        # Target intensity increase per step
                                       # Reward = 0 at ΔI = target (threshold)
                                       # Reward > 0 only when ΔI > target
    
    # Substrate-aware normalization (bounds distance reward to [-1, 1])
    substrate_aware_scaling: true      # Use substrate gradient for scaling
    use_tanh_normalization: true       # Use tanh (true) or softsign (false)
    tanh_scale: 2.5                    # Scale factor: c = tanh_scale × target_delta_intensity
                                       # This ensures tanh(target_delta_intensity / c) = tanh(1) ≈ 0.76
                                       # For threshold at target: set c = target_delta_intensity (tanh(1)=0.76)
                                       # For zero at target: use shifted tanh or adjust calculation
    gradient_cap: null                 # Optional: cap gradient for exponential substrates (e.g., 10.0)
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: false
      shaping_coeff: 0.05             # Lowest among all components
      phi_distance_scale: 1.0         # Scale for distance-to-goal potential
  
  # Termination Reward (Priority 4: Sparse terminal signals)
  # Sparse binary rewards: +1 for success, -1 for failures
  # Applied only at episode termination
  termination_rewards:
    success_reward: 1.0                      # Node reaches rightmost area
    critical_nodes_penalty: -1.0             # Too many nodes (> threshold)
    no_nodes_penalty: -1.0                   # Graph collapses to 0 nodes
    out_of_bounds_penalty: -1.0              # Node escapes substrate bounds
    leftward_drift_penalty: -1.0             # Centroid drifts left consecutively
    timeout_penalty: 0.0                     # Max steps reached (neutral)
  
  # ============================================================================
  # OBSERVATION SELECTION (Deletion-Aware)
  # ============================================================================
  # Prioritizes nodes important for deletion decisions:
  # - Rightward nodes (x-coordinate): spatial progress
  # - High-intensity nodes: candidates for marking (low intensity = marked)
  # - Representationally important nodes: high embedding norm
  observation_selection:
    method: topk_x
    w_x: 0.6           # Rightward bias (durotaxis direction)
    w_intensity: 0.3   # Intensity-based selection (marking criterion)
    w_norm: 0.1        # Representational importance

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
encoder:
  out_dim: 128
  num_layers: 4

actor_critic:
  hidden_dim: 128
  continuous_dim: 5
  dropout_rate: 0.2
  pretrained_weights: 'imagenet'
  
  # Backbone configuration
  backbone:
    input_adapter: 1ch_conv
    freeze_mode: all
    backbone_lr: 1.0e-4
    head_lr: 3.0e-4
  
  # Action parameter bounds
  action_parameter_bounds:
    delete_ratio: [0.0, 0.7]
    gamma: [0.5, 5.0]
    alpha: [0.5, 3.0]
    noise: [0.05, 0.10]
    theta: [-0.1745, 0.1745]  # -10 to 10 deg
  
  # Value function components
  value_components:
    - 'total_reward'
    - 'delete_reward'
    - 'distance_reward'
    - 'termination_reward'
  
  # Simplicial Embedding
  simplicial_embedding:
    enabled: true
    num_groups: 16
    temperature: 1.0

# ============================================================================
# TRAINING ALGORITHM
# ============================================================================
algorithm:
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  clip_epsilon: 0.2
  value_loss_coeff: 0.25
  max_grad_norm: 0.5
  target_kl: 0.03

# ============================================================================
# TRAINER CONFIGURATION
# ============================================================================
trainer:
  substrate_type: random
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.0001
  save_dir: "./training_results"
  
  # Resume training
  resume_training:
    enabled: true
    checkpoint_path: "training_results/run0043/succ_model_batch24.pt"
    resume_from_best: false
    reset_optimizer: false
    reset_episode_count: true
  
  # Model selection
  model_selection:
    primary_metric: success_rate
    primary_weight: 1000.0
    secondary_metric: progress
    secondary_weight: 100.0
    tertiary_metric: return_mean
    tertiary_weight: 1.0
    window_episodes: 50
    min_improvement: 0.01
    min_episode_for_best_save: 20
  
  # Learning rate schedule
  warmup_updates_for_best: 5
  lr_warmup_updates: 200
  lr_total_updates: 10000
  
  # Batch training
  rollout_collection_mode: "steps"
  rollout_steps: 2048
  rollout_batch_size: 10
  update_epochs: 4
  minibatch_size: 64
  
  # Value clipping
  enable_value_clipping: true
  value_clip_epsilon: 0.2
  use_relative_value_clip: true 
  
  # Return normalization (prevents value loss explosion)
  normalize_returns: true
  return_scale: 10.0
  
  # Logging
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null
  
  # Detailed logging
  detailed_logging:
    enable_detailed_logging: true
    save_detailed_logs_to_json: true
    single_file_mode: true
    log_node_positions: true
    log_spawn_parameters: true
    log_persistent_ids: true
    log_connectivity: false
    log_substrate_values: false
    compress_logs: false
    max_log_file_size_mb: 50
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]           # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]            # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]     # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]       # Base value range for exponential substrate
  
  # Optimization
  entropy_bonus_coeff: 0.1
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Component weights (for multi-head value function)
  # Normalized to sum to 1.0 for balanced critic head learning
  component_weights:
    total_reward: 0.3
    delete_reward: 0.3
    distance_reward: 0.3
    termination_reward: 0.1
  
  # Policy loss weights
  policy_loss_weights:
    continuous_weight: 1.0
    entropy_weight: 0.01
  
  # Gradient scaling
  gradient_scaling:
    enable_adaptive_scaling: true
    gradient_norm_target: 1.0
    scaling_momentum: 0.9
    min_scaling_factor: 0.1
    max_scaling_factor: 10.0
    warmup_steps: 100
  
  # Adaptive scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
  
  # Reward normalization
  reward_normalization:
    enable_per_episode_norm: true
    enable_cross_episode_scaling: true
    min_episode_length: 3
    normalization_method: 'zscore'
  
  # Advantage weighting
  advantage_weighting:
    enable_learnable_weights: true
    enable_attention_weighting: true
    weight_learning_rate: 0.01
    weight_regularization: 0.001
  
  # Entropy regularization
  entropy_regularization:
    enable_adaptive_entropy: true
    entropy_coeff_start: 0.5
    entropy_coeff_end: 0.1
    entropy_decay_episodes: 500
    continuous_entropy_weight: 1.0
    min_entropy_threshold: 0.5

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================
system:
  device: 'auto'
  num_workers: 1
  seed: null

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  tensorboard: false
  wandb: false
  save_model_every: null
  save_best_model: true
  verbose: true
