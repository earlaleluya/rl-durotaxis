# Configuration file for Durotaxis Reinforcement Learning Training
# This file contains default parameters for all necessary classes used in DurotaxisTrainer

# ============================================================================
# DurotaxisTrainer Configuration
# ============================================================================
trainer:
  # Training parameters
  # Total number of episodes for the whole training run.
  # Note: When `curriculum_learning.enable_curriculum` is true the trainer will
  # automatically scale curriculum stage durations to this value. If you change
  # this number (for example to 10000), the curriculum stages will be resized
  # proportionally so the curriculum covers the full training run.
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.0001   
  save_dir: "./training_results"
  
  # ============================================================================
  # CURRICULUM LEARNING SYSTEM - Progressive Migration Training
  # ============================================================================
  # This curriculum works WITH the new reward structure to progressively
  # guide the agent from basic survival to successful rightmost migration
  curriculum_learning:
    enable_curriculum: true           # Enable progressive learning stages
    
    # Stage 1: Survival & Basic Movement (Episodes 0-200)
    # GOAL: Learn to stay alive and move rightward without catastrophic failures
    stage_1_navigation:
      episode_start: 0
      episode_end: 200
      focus: "survival_and_movement"
      description: "Master survival, avoid boundaries, and learn rightward movement"
      
      # Simplified environment for early learning
      max_nodes_allowed: 30           # Moderate complexity (was 5, too restrictive)
      simplified_actions: false       # Keep full actions to learn properly
      
      # Stage 1: Emphasize survival and basic movement
      reward_multipliers:
        survival_reward: 3.0          # 3x survival bonus (learn to stay alive)
        movement_reward: 2.0          # 2x movement reward (learn direction)
        centroid_movement_reward: 2.0 # 2x centroid reward (collective movement)
        boundary_penalty: 5.0         # 5x boundary penalty (avoid catastrophe)
        milestone_rewards: 2.0        # 2x milestone rewards (celebrate progress)
        
      # Success criteria for stage 1
      success_criteria:
        min_episode_length: 50        # Survive at least 50 steps
        max_boundary_violations: 2    # Allow some exploration mistakes
        min_rightward_progress: 50.0  # Must reach 50 units right (approx 8% of 600)
        min_centroid_x: 100.0         # Centroid must reach x=100
        
    # Stage 2: Distance Milestones (Episodes 201-500)  
    # GOAL: Reach progressive distance milestones (25%, 50%, 75%)
    stage_2_management:
      episode_start: 201
      episode_end: 500
      focus: "milestone_achievement"
      description: "Reach 25%, 50%, and 75% distance milestones consistently"
      
      # Increased complexity for intermediate learning
      max_nodes_allowed: 50           # Standard complexity (was 15)
      unlock_advanced_actions: true   # Full action space
      
      # Stage 2: Emphasize milestone achievement and sustained movement
      reward_multipliers:
        survival_reward: 2.0          # Still important but reduced
        movement_reward: 1.5          # Still rewarded
        centroid_movement_reward: 2.5 # 2.5x centroid (group progress crucial)
        milestone_rewards: 3.0        # 3x milestone rewards (primary focus!)
        spawn_success_reward: 1.5     # 1.5x spawn (efficient expansion)
        intensity_bonus: 2.0          # 2x intensity bonus (seek better substrate)
        
      # Success criteria for stage 2
      success_criteria:
        min_episode_length: 100       # Longer episodes required
        max_boundary_violations: 1    # Fewer mistakes allowed
        min_rightward_progress: 150.0 # Must reach 150 units (25% of 600)
        milestone_25_percent: true    # Must trigger 25% milestone
        milestone_50_percent: true    # Aim for 50% milestone
        
    # Stage 3: Goal Achievement (Episodes 501-1000)
    # GOAL: Consistently reach the rightmost substrate (100% success)
    stage_3_optimization:
      episode_start: 501
      episode_end: 1000
      focus: "goal_completion"
      description: "Master full migration to rightmost substrate"
      
      # Full complexity for final mastery
      max_nodes_allowed: 75           # Maximum capacity (was 50)
      enable_all_constraints: true    # Full problem complexity
      
      # Stage 3: Balanced rewards with emphasis on goal completion
      reward_multipliers:
        survival_reward: 1.5          # Reduced but still present
        movement_reward: 1.0          # Standard (well-learned by now)
        centroid_movement_reward: 2.0 # Still important for group coordination
        milestone_rewards: 1.5        # Reduced (should be easy now)
        success_reward: 2.0           # 2x success reward (MAIN GOAL!)
        spawn_success_reward: 1.2     # Slightly boosted for efficiency
        efficiency_bonus: 3.0         # 3x efficiency (optimize path)
        
      # Success criteria for stage 3  
      success_criteria:
        min_episode_length: 200       # Long sustainable episodes
        max_boundary_violations: 0    # No violations in final stage
        min_rightward_progress: 450.0 # Must reach 450+ units (75%+)
        completion_rate: 0.15         # 15% success rate to advance
        milestone_90_percent: true    # Must trigger 90% milestone
        efficiency_threshold: 0.7     # 70% efficiency required
        
  # Curriculum progression settings - ALIGNED WITH NEW REWARD STRUCTURE
  curriculum_progression:
    auto_advance: true                # Automatically advance stages
    advancement_criteria: "mixed"     # Use both success rate AND episode thresholds
    min_success_rate: 0.10           # 10% success rate to advance (was 60%, too strict)
    evaluation_window: 100           # Evaluate over last 100 episodes (more stable)
    allow_early_advance: true        # Allow advancing if criteria met before episode_end
    force_advance_at_end: true       # Always advance at episode_end regardless of performance
    # Overlap control between stages:
    # - You may provide `stage_overlap` (absolute number of episodes to overlap)
    # - OR provide `curriculum_overlap_pct` (fraction 0..1) which will be applied
    #   proportionally to each stage after scaling. When both are present, the
    #   fractional option `curriculum_overlap_pct` takes precedence.
    # Example: curriculum_overlap_pct: 0.02 -> 2% overlap of each stage length
    stage_overlap: 50                # 50 episode overlap for gradual transition
    # Optional proportional overlap (fraction of stage length). Use instead of stage_overlap.
    # curriculum_overlap_pct: 0.05
    
  # Reward shaping system - ALIGNED WITH MIGRATION GOAL
  reward_shaping:
    enable_progressive_shaping: true  # Enable progressive reward shaping
    stage_specific_bonuses: true     # Enable stage-specific bonus rewards
    milestone_rewards: true          # Enable milestone achievement rewards
    
    # Milestone definitions - FOCUSED ON MIGRATION PROGRESS
    milestones:
      first_survival_50_steps: 30.0        # Bonus for first 50+ step episode (Stage 1)
      first_survival_100_steps: 50.0       # Bonus for first 100+ step episode (Stage 2)
      first_rightward_50_units: 40.0       # Bonus for reaching 50 units right (Stage 1)
      first_rightward_150_units: 60.0      # Bonus for reaching 150 units (25%) (Stage 2)
      first_rightward_300_units: 100.0     # Bonus for reaching 300 units (50%) (Stage 2)
      first_rightward_450_units: 150.0     # Bonus for reaching 450 units (75%) (Stage 3)
      boundary_violation_free: 40.0        # Bonus for episode without violations
      first_milestone_25_percent: 80.0     # Bonus for triggering first 25% milestone
      first_milestone_50_percent: 120.0    # Bonus for triggering first 50% milestone
      first_milestone_75_percent: 180.0    # Bonus for triggering first 75% milestone
      first_milestone_90_percent: 250.0    # Bonus for triggering first 90% milestone
      task_completion: 300.0               # HUGE bonus for first goal completion!
  
  # Loss weighting and optimization
  entropy_bonus_coeff: 0.02        # INCREASED from 0.01 for more exploration bonus
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Batch training configuration
  rollout_batch_size: 10      # Number of episodes to collect before updating
  update_epochs: 4            # Number of update epochs per batch
  minibatch_size: 64          # Size of minibatches during updates
  
  # Value clipping configuration
  enable_value_clipping: true # Enable PPO-style value clipping for stable training
  value_clip_epsilon: 0.2     # Clipping range for value predictions (0.1-0.3 typical)
  
  # Logging and checkpointing
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null  # Set to integer for periodic checkpoints, null for disabled
  
  # Detailed node logging configuration
  detailed_logging:
    enable_detailed_logging: true      # Enable comprehensive node tracking per step
    save_detailed_logs_to_json: true   # Save detailed logs to JSON files
    single_file_mode: true             # Save all episodes to one JSON file (vs separate files)
    log_node_positions: true           # Include node x,y positions
    log_spawn_parameters: true         # Include gamma, alpha, noise, theta values
    log_persistent_ids: true           # Include persistent node IDs
    log_connectivity: false            # Include degree and neighbor information
    log_substrate_values: false        # Include substrate values at node positions
    compress_logs: false               # Compress JSON files (reduces file size)
    max_log_file_size_mb: 50           # Maximum size per log file in MB
  
  # Environment setup parameters
  substrate_type: 'random'  # 'linear', 'exponential', or 'random'
  
  # Empty graph handling configuration
  empty_graph_handling:
    enable_graceful_recovery: true  # Enable graceful handling instead of breaking episodes
    recovery_num_nodes: 5           # Number of nodes to spawn when recovering from empty graph
    log_recoveries: true            # Log when empty graph recovery occurs
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]    # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]     # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]  # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]    # Base value range for exponential substrate
  
  # Component weights for multi-objective learning (adaptive)
  component_weights:
    total_reward: 1.0
    graph_reward: 0.4
    spawn_reward: 0.3
    delete_reward: 0.2
    edge_reward: 0.2
    total_node_reward: 0.3
  
  # Hybrid policy loss weights with gradient scaling
  policy_loss_weights:
    discrete_weight: 0.7     # Weight for discrete actions (spawn/delete)
    continuous_weight: 0.3   # Weight for continuous parameters
    entropy_weight: 0.01     # Exploration bonus weight
  
  # Adaptive gradient scaling for hybrid action spaces
  gradient_scaling:
    enable_adaptive_scaling: true    # Enable intelligent gradient balancing
    gradient_norm_target: 1.0        # Target gradient norm for balancing
    scaling_momentum: 0.9            # EMA momentum for gradient norm tracking
    min_scaling_factor: 0.1          # Minimum scaling to prevent collapse
    max_scaling_factor: 10.0         # Maximum scaling to prevent explosion
    warmup_steps: 100                # Steps to establish baseline gradient norms
  
  # Adaptive reward scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
    
  # Reward normalization system (two-tier approach) - STRENGTHENED
  reward_normalization:
    enable_per_episode_norm: true     # Tier 1: Per-episode normalization
    enable_cross_episode_scaling: true  # Tier 2: Cross-episode adaptive scaling
    min_episode_length: 3             # Reduced for more frequent normalization
    normalization_method: 'zscore'    # Changed to zscore for stronger normalization
    
  # Enhanced learnable advantage weighting system
  advantage_weighting:
    enable_learnable_weights: true    # Enable learnable component weights
    enable_attention_weighting: true  # Enable attention-based dynamic weighting
    weight_learning_rate: 0.01        # Learning rate for component weights
    weight_regularization: 0.001      # L2 regularization for weight stability
    
  # Enhanced entropy regularization system - INCREASED FOR MORE EXPLORATION
  entropy_regularization:
    enable_adaptive_entropy: true     # Enable adaptive entropy scheduling
    entropy_coeff_start: 0.8          # GREATLY INCREASED from 0.5 for maximum exploration
    entropy_coeff_end: 0.05           # INCREASED from 0.02 to maintain exploration longer
    entropy_decay_episodes: 180       # Increased for even slower decay
    discrete_entropy_weight: 3.0      # INCREASED from 2.0 for more discrete exploration
    continuous_entropy_weight: 2.0    # INCREASED from 1.5 for more continuous exploration
    min_entropy_threshold: 0.2        # INCREASED from 0.1 to prevent early collapse

# ============================================================================
# DurotaxisEnv Configuration
# ============================================================================
environment:
  # Basic environment parameters - EXPANDED for successful migration
  substrate_size: [600, 400]  # Much larger substrate (3x width)
  substrate_type: 'linear'
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology parameters - RELAXED for learning
  init_num_nodes: 5              # Start with more nodes for stability
  max_critical_nodes: 75         # Allow more nodes
  threshold_critical_nodes: 500  # Higher threshold before termination
  max_steps: 1000                # Much more time to reach goal
  
  # Termination behavior - RELAXED for exploration
  consecutive_left_moves_limit: 30  # More forgiving (was implicitly 6)
  
  # ============================================================================
  # CURRICULUM-SPECIFIC ENVIRONMENT SETTINGS
  # ============================================================================
  curriculum_environment:
    # Stage 1: Simplified environment for navigation learning
    stage_1_settings:
      simplified_substrate: true      # Use simpler substrate patterns
      reduced_action_space: true      # Limit available actions
      boundary_buffer: 10.0           # Add safety buffer near boundaries
      max_nodes_override: 5           # Override max nodes for stage 1
      disable_complex_rewards: true   # Disable complex reward components
      
    # Stage 2: Moderate complexity for node management
    stage_2_settings:
      enable_advanced_spawning: true  # Enable more spawning options
      node_management_focus: true     # Focus on node lifecycle management
      max_nodes_override: 15          # Override max nodes for stage 2
      enable_deletion_training: true  # Special deletion training mode
      
    # Stage 3: Full complexity
    stage_3_settings:
      full_environment: true          # Enable all features
      competition_mode: true          # Enable competitive constraints
      optimization_focus: true        # Focus on efficiency metrics
      enable_success_detection: true  # Enable task completion detection
  
  # Simulation parameters
  delta_time: 3
  delta_intensity: 2.50
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # Reward structure - Graph rewards (OPTIMIZED for migration success)
  graph_rewards:
    connectivity_penalty: 5.0     # Moderate penalty for losing connectivity
    growth_penalty: 5.0           # Moderate penalty for excessive growth
    survival_reward: 0.1          # Reward for each step survived
    action_reward: 0.01           # Small reward for taking actions
    centroid_movement_reward: 2.0 # NEW: Strong reward for collective rightward movement
  
  # Reward structure - Node rewards (OPTIMIZED for migration)
  node_rewards:
    movement_reward: 2.0          # HUGE rightward movement incentive
    leftward_penalty: 1.0         # NEW: Penalty for leftward movement
    intensity_penalty: 0.5        # REDUCED 75%: Very gentle for exploration
    intensity_bonus: 1.5          # INCREASED 15x: Strong bonus for good positions
    substrate_reward: 0.3         # Increased substrate interaction reward
  
  # Curriculum-specific reward bonuses
  curriculum_rewards:
    # Stage 1: Navigation bonuses
    navigation_bonuses:
      rightward_movement_bonus: 1.0     # Extra bonus for moving right
      boundary_avoidance_bonus: 2.0     # Extra bonus for avoiding boundaries
      episode_survival_bonus: 0.5       # Bonus per step survived
      distance_milestone_bonus: 5.0     # Bonus for distance milestones
      
    # Stage 2: Node management bonuses  
    management_bonuses:
      successful_spawn_bonus: 2.0       # Bonus for successful node spawning
      efficient_deletion_bonus: 3.0     # Bonus for efficient node deletion
      node_diversity_bonus: 1.5         # Bonus for maintaining node diversity
      connectivity_maintenance_bonus: 1.0  # Bonus for good connectivity
      
    # Stage 3: Optimization bonuses
    optimization_bonuses:
      task_completion_bonus: 50.0       # Large bonus for completing task
      efficiency_bonus: 10.0            # Bonus for efficient performance
      constraint_satisfaction_bonus: 5.0  # Bonus for satisfying all constraints
      perfection_bonus: 25.0            # Bonus for perfect execution
  
  # Reward structure - Edge rewards
  edge_reward:
    rightward_bonus: 0.5          # INCREASED from 0.1 for better directional incentive
    leftward_penalty: 0.05        # REDUCED from 0.1 for gentler penalty
  
  # Reward structure - Spawn rewards (ENHANCED for boundary-safe spawning)
  spawn_rewards:
    spawn_success_reward: 1.0     # Reward for successful durotaxis spawning
    spawn_failure_penalty: 1.0    # Penalty for failed durotaxis spawning
    
    # NEW: Boundary-aware spawn penalties
    spawn_near_boundary_penalty: 3.0  # Strong penalty for spawning nodes near boundaries
    spawn_in_danger_zone_penalty: 8.0 # SEVERE penalty for spawning in danger zone
    spawn_boundary_check: true    # Enable boundary checking for new spawns
  
  # Reward structure - Delete rewards
  delete_reward:
    proper_deletion: 2.0          # Reward for proper node deletion
    persistence_penalty: 2.0      # Penalty for inappropriate persistence
  
  # Reward structure - Position rewards (ENHANCED for boundary avoidance)
  position_rewards:
    boundary_bonus: 0.1           # Bonus for boundary/frontier nodes (horizontal edges only)
    left_edge_penalty: 0.2        # Penalty for being near left edge
    
    # ENHANCED: Progressive penalties for approaching top/bottom boundaries
    edge_position_penalty: 0.5    # Base penalty for being near top/bottom edges (INCREASED 5x)
    danger_zone_penalty: 2.0      # NEW: Strong penalty for danger zone (very close to boundary)
    critical_zone_penalty: 5.0    # NEW: SEVERE penalty for critical zone (about to violate)
    
    # Boundary zone thresholds (as percentage of substrate height)
    edge_zone_threshold: 0.15     # Within 15% of top/bottom = edge zone
    danger_zone_threshold: 0.08   # Within 8% of top/bottom = danger zone  
    critical_zone_threshold: 0.03 # Within 3% of top/bottom = critical zone
    
    # Safe zone rewards (encourage staying in center)
    safe_center_bonus: 0.05       # NEW: Small bonus for staying in safe center zone
    safe_center_range: 0.30       # Center 30% of height is "safe zone"
  
  # Reward structure - Termination rewards (OPTIMIZED for learning)
  termination_rewards:
    success_reward: 500.0         # MASSIVE reward for reaching goal (5x increase)
    out_of_bounds_penalty: -100.0 # SEVERE penalty to discourage boundary violations
    no_nodes_penalty: -100.0      # SEVERE penalty to discourage losing all nodes
    leftward_drift_penalty: -50.0 # Strong penalty for persistent leftward movement
    timeout_penalty: 0.0          # NO penalty for timeout (exploration is good!)
    critical_nodes_penalty: -15.0 # Moderate penalty
  
  # NEW: Progressive milestone rewards for reaching distance thresholds
  milestone_rewards:
    enabled: true
    distance_25_percent: 25.0     # Reward for reaching 25% of substrate width
    distance_50_percent: 50.0     # Reward for reaching 50% of substrate width
    distance_75_percent: 100.0    # Reward for reaching 75% of substrate width
    distance_90_percent: 200.0    # Huge reward for almost reaching goal
  
  # NEW: Time-based survival rewards (encourage longevity)
  survival_reward_config:
    enabled: true
    base_reward: 0.02            # Reward per step for staying alive
    bonus_threshold: 100         # After 100 steps, give bonus
    bonus_reward: 0.05           # Additional reward after threshold
    max_step_factor: 0.8         # Scale rewards up to 80% of max_steps

# ============================================================================
# GraphInputEncoder Configuration
# ============================================================================
encoder:
  out_dim: 64                   # Output embedding dimension per node
  num_layers: 4                 # Number of transformer layers

# ============================================================================
# HybridActorCritic Configuration
# ============================================================================
actor_critic:
  hidden_dim: 128               # Hidden dimension for actor-critic networks
  num_discrete_actions: 2       # Number of discrete actions (spawn/delete)
  continuous_dim: 4             # Dimension of continuous action space (gamma, alpha, noise, theta)
  dropout_rate: 0.1             # Dropout rate for regularization
  
  # Spawn parameter bounds [min, max]
  spawn_parameter_bounds:
    gamma: [2.0, 8.0]          # Cell migration sensitivity parameter
    alpha: [0.5, 4.0]           # Directional bias strength
    noise: [0.05, 2.0]           # Stochastic noise level
    theta: [-1.5708, 1.5708]    # Migration direction angle (-pi/2 to pi/2)
  
  # Value function components (list of reward components to predict)
  value_components:
    - 'total_reward'
    - 'graph_reward'
    - 'spawn_reward'
    - 'delete_reward'
    - 'edge_reward'
    - 'total_node_reward'

# ============================================================================
# Training Algorithm Configuration
# ============================================================================
algorithm:
  # GAE (Generalized Advantage Estimation) parameters
  gamma: 0.99                   # Discount factor
  gae_lambda: 0.95              # GAE lambda parameter
  
  # PPO (Proximal Policy Optimization) parameters
  ppo_epochs: 4                 # Number of PPO epochs per update
  clip_epsilon: 0.1             # PPO clipping parameter
  value_loss_coeff: 0.5         # Increased from 0.5 for better value estimation
  
  # Optimization parameters
  max_grad_norm: 0.5            # Increased from 0.5 for less aggressive clipping

# ============================================================================
# Device and Performance Configuration
# ============================================================================
system:
  device: 'auto'                # 'cuda', 'cpu', or 'auto' for automatic detection
  num_workers: 1                # Number of parallel workers (if applicable)
  seed: null                    # Random seed (null for no seeding)

# ============================================================================
# Logging and Monitoring Configuration
# ============================================================================
logging:
  tensorboard: false            # Enable TensorBoard logging
  wandb: false                  # Enable Weights & Biases logging
  save_model_every: null        # Save model every N episodes (null for disabled)
  save_best_model: true         # Save best performing model
  verbose: true                 # Verbose logging

# ============================================================================
# Experimental Configuration
# ============================================================================
experimental:
  # Action masking
  use_action_masking: true      # Enable action masking for invalid actions
  
  # Adaptive component weighting
  adaptive_component_weights: true    # Enable adaptive component weight updates
  
  # Substrate variation
  substrate_variation: true     # Enable substrate parameter variation during training
  
  # CURRICULUM LEARNING - Now using advanced 3-stage system from trainer section
  # The old simple 3-phase system is DEPRECATED and replaced by the comprehensive
  # curriculum_learning configuration in the trainer section above.
  # That system includes reward multipliers, milestone tracking, and adaptive progression.
  curriculum_learning:
    enable_curriculum: false          # DISABLED: Use trainer curriculum instead
    # Legacy phase configuration kept for reference but not used
    phase_1_episodes: 60
    phase_2_episodes: 120
    phase_1_config:
      max_critical_nodes: 30
      init_num_nodes: 3
      no_nodes_penalty_multiplier: 0.5
    phase_2_config:
      max_critical_nodes: 40
      init_num_nodes: 2
      no_nodes_penalty_multiplier: 0.75
    phase_3_config:
      max_critical_nodes: 50
      init_num_nodes: 1
      no_nodes_penalty_multiplier: 1.0
  
  # NEW: Enhanced success criteria
  success_criteria:
    enable_multiple_criteria: true    # Use multiple success definitions
    survival_success_steps: 10        # Maintain nodes for 10+ steps
    reward_success_threshold: -20     # Achieve reasonable reward
    growth_success_nodes: 2           # Maintain connectivity (2+ nodes)
    exploration_success_steps: 15     # Long episode regardless of outcome
