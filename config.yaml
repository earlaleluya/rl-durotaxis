# Configuration file for Durotaxis Reinforcement Learning Training
# This file contains default parameters for all necessary classes used in DurotaxisTrainer

# ============================================================================
# DurotaxisTrainer Configuration
# ============================================================================
trainer:
  # Training parameters
  total_episodes: 150       # Reduced for curriculum testing
  max_steps: 1000
  learning_rate: 0.00008   # Slightly increased for faster learning with better exploration
  save_dir: "./training_results"
  
  # ============================================================================
  # CURRICULUM LEARNING SYSTEM - Progressive Skill Development
  # ============================================================================
  curriculum_learning:
    enable_curriculum: true           # Enable progressive learning stages
    
    # Stage 1: Navigation Fundamentals (Episodes 0-50)
    stage_1_navigation:
      episode_start: 0
      episode_end: 50
      focus: "basic_navigation"
      description: "Learn to move from left to right without hitting boundaries"
      
      # Simplified environment for stage 1
      max_nodes_allowed: 5            # Limit complexity
      simplified_actions: true        # Reduce action space complexity
      
      # Stage 1 specific rewards (emphasize navigation)
      reward_multipliers:
        movement_reward: 5.0          # 5x bonus for rightward movement
        boundary_penalty: 10.0        # Heavy penalty for boundary violations
        survival_reward: 2.0          # 2x bonus for staying alive
        node_count_penalty: 0.1       # Minimal penalty for node management
        
      # Success criteria for stage 1
      success_criteria:
        min_episode_length: 10        # Must survive at least 10 steps
        max_boundary_violations: 0    # No boundary violations allowed
        min_rightward_progress: 5.0   # Must move at least 5 units right
        
    # Stage 2: Node Management (Episodes 51-100)  
    stage_2_management:
      episode_start: 51
      episode_end: 100
      focus: "spawn_delete_control"
      description: "Learn efficient spawning and deletion of nodes"
      
      # Moderate complexity for stage 2
      max_nodes_allowed: 15           # Allow more nodes
      unlock_advanced_actions: true   # Enable full action space
      
      # Stage 2 specific rewards (emphasize node control)
      reward_multipliers:
        movement_reward: 2.0          # Reduced navigation focus
        spawn_success_reward: 3.0     # 3x bonus for successful spawning
        deletion_efficiency: 4.0      # 4x bonus for efficient deletion
        node_diversity_bonus: 2.0     # Bonus for maintaining diverse nodes
        connectivity_bonus: 1.5       # Bonus for good connectivity
        
      # Success criteria for stage 2
      success_criteria:
        min_episode_length: 20        # Longer episodes required
        max_boundary_violations: 1    # Allow occasional violations
        min_spawn_actions: 3          # Must spawn at least 3 nodes
        min_delete_actions: 2         # Must delete at least 2 nodes
        
    # Stage 3: Optimization (Episodes 101-150)
    stage_3_optimization:
      episode_start: 101
      episode_end: 150
      focus: "full_optimization"
      description: "Master all constraints and optimize performance"
      
      # Full complexity for stage 3
      max_nodes_allowed: 50           # No restrictions
      enable_all_constraints: true    # Full problem complexity
      
      # Stage 3 balanced rewards (full optimization)
      reward_multipliers:
        movement_reward: 1.0          # Standard navigation rewards
        spawn_success_reward: 1.0     # Standard spawning rewards
        deletion_efficiency: 1.0      # Standard deletion rewards
        success_completion: 10.0      # 10x bonus for task completion
        efficiency_bonus: 5.0         # 5x bonus for efficiency metrics
        
      # Success criteria for stage 3  
      success_criteria:
        min_episode_length: 50        # Long episodes required
        max_boundary_violations: 0    # No violations in final stage
        completion_rate: 0.1          # 10% task completion rate
        efficiency_threshold: 0.8     # 80% efficiency required
        
  # Curriculum progression settings
  curriculum_progression:
    auto_advance: true                # Automatically advance stages
    advancement_criteria: "success_rate"  # Advance based on success rate
    min_success_rate: 0.6            # 60% success rate to advance
    evaluation_window: 50            # Evaluate over last 50 episodes
    stage_overlap: 20                # 20 episode overlap between stages
    
  # Reward shaping system
  reward_shaping:
    enable_progressive_shaping: true  # Enable progressive reward shaping
    stage_specific_bonuses: true     # Enable stage-specific bonus rewards
    milestone_rewards: true          # Enable milestone achievement rewards
    
    # Milestone definitions
    milestones:
      first_successful_navigation: 50.0    # Bonus for first successful navigation
      first_successful_spawn: 30.0         # Bonus for first successful spawn
      first_successful_deletion: 25.0      # Bonus for first successful deletion
      first_long_episode: 40.0             # Bonus for first 30+ step episode
      boundary_violation_free: 60.0        # Bonus for episode without violations
      task_completion: 100.0               # Bonus for completing the task
  
  # Loss weighting and optimization
  entropy_bonus_coeff: 0.1        # INCREASED from 0.01 for more exploration bonus
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Batch training configuration
  rollout_batch_size: 10      # Number of episodes to collect before updating
  update_epochs: 4            # Number of update epochs per batch
  minibatch_size: 64          # Size of minibatches during updates
  
  # Value clipping configuration
  enable_value_clipping: true # Enable PPO-style value clipping for stable training
  value_clip_epsilon: 0.2     # Clipping range for value predictions (0.1-0.3 typical)
  
  # Logging and checkpointing
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null  # Set to integer for periodic checkpoints, null for disabled
  
  # Environment setup parameters
  substrate_type: 'random'  # 'linear', 'exponential', or 'random'
  
  # Empty graph handling configuration
  empty_graph_handling:
    enable_graceful_recovery: true  # Enable graceful handling instead of breaking episodes
    recovery_num_nodes: 1           # Number of nodes to spawn when recovering from empty graph
    log_recoveries: true            # Log when empty graph recovery occurs
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]    # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]     # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]  # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]    # Base value range for exponential substrate
  
  # Component weights for multi-objective learning (adaptive)
  component_weights:
    total_reward: 1.0
    graph_reward: 0.4
    spawn_reward: 0.3
    delete_reward: 0.2
    edge_reward: 0.2
    total_node_reward: 0.3
  
  # Hybrid policy loss weights with gradient scaling
  policy_loss_weights:
    discrete_weight: 0.7     # Weight for discrete actions (spawn/delete)
    continuous_weight: 0.3   # Weight for continuous parameters
    entropy_weight: 0.01     # Exploration bonus weight
  
  # Adaptive gradient scaling for hybrid action spaces
  gradient_scaling:
    enable_adaptive_scaling: true    # Enable intelligent gradient balancing
    gradient_norm_target: 1.0        # Target gradient norm for balancing
    scaling_momentum: 0.9            # EMA momentum for gradient norm tracking
    min_scaling_factor: 0.1          # Minimum scaling to prevent collapse
    max_scaling_factor: 10.0         # Maximum scaling to prevent explosion
    warmup_steps: 100                # Steps to establish baseline gradient norms
  
  # Adaptive reward scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
    
  # Reward normalization system (two-tier approach) - STRENGTHENED
  reward_normalization:
    enable_per_episode_norm: true     # Tier 1: Per-episode normalization
    enable_cross_episode_scaling: true  # Tier 2: Cross-episode adaptive scaling
    min_episode_length: 3             # Reduced for more frequent normalization
    normalization_method: 'zscore'    # Changed to zscore for stronger normalization
    
  # Enhanced learnable advantage weighting system
  advantage_weighting:
    enable_learnable_weights: true    # Enable learnable component weights
    enable_attention_weighting: true  # Enable attention-based dynamic weighting
    weight_learning_rate: 0.01        # Learning rate for component weights
    weight_regularization: 0.001      # L2 regularization for weight stability
    
  # Enhanced entropy regularization system - INCREASED FOR MORE EXPLORATION
  entropy_regularization:
    enable_adaptive_entropy: true     # Enable adaptive entropy scheduling
    entropy_coeff_start: 0.8          # GREATLY INCREASED from 0.5 for maximum exploration
    entropy_coeff_end: 0.05           # INCREASED from 0.02 to maintain exploration longer
    entropy_decay_episodes: 180       # Increased for even slower decay
    discrete_entropy_weight: 3.0      # INCREASED from 2.0 for more discrete exploration
    continuous_entropy_weight: 2.0    # INCREASED from 1.5 for more continuous exploration
    min_entropy_threshold: 0.2        # INCREASED from 0.1 to prevent early collapse

# ============================================================================
# DurotaxisEnv Configuration
# ============================================================================
environment:
  # Basic environment parameters
  substrate_size: [200, 200]
  substrate_type: 'linear'
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology parameters
  init_num_nodes: 1
  max_critical_nodes: 50
  threshold_critical_nodes: 200
  
  # ============================================================================
  # CURRICULUM-SPECIFIC ENVIRONMENT SETTINGS
  # ============================================================================
  curriculum_environment:
    # Stage 1: Simplified environment for navigation learning
    stage_1_settings:
      simplified_substrate: true      # Use simpler substrate patterns
      reduced_action_space: true      # Limit available actions
      boundary_buffer: 10.0           # Add safety buffer near boundaries
      max_nodes_override: 5           # Override max nodes for stage 1
      disable_complex_rewards: true   # Disable complex reward components
      
    # Stage 2: Moderate complexity for node management
    stage_2_settings:
      enable_advanced_spawning: true  # Enable more spawning options
      node_management_focus: true     # Focus on node lifecycle management
      max_nodes_override: 15          # Override max nodes for stage 2
      enable_deletion_training: true  # Special deletion training mode
      
    # Stage 3: Full complexity
    stage_3_settings:
      full_environment: true          # Enable all features
      competition_mode: true          # Enable competitive constraints
      optimization_focus: true        # Focus on efficiency metrics
      enable_success_detection: true  # Enable task completion detection
  
  # Simulation parameters
  delta_time: 3
  delta_intensity: 2.50
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # Reward structure - Graph rewards (Base values - modified by curriculum)
  graph_rewards:
    connectivity_penalty: 2.0     # Base penalty (curriculum multipliers apply)
    growth_penalty: 2.0           # Base penalty (curriculum multipliers apply) 
    survival_reward: 0.5          # Base reward (curriculum multipliers apply)
    action_reward: 0.1            # Base reward (curriculum multipliers apply)
  
  # Reward structure - Node rewards (Base values - modified by curriculum)
  node_rewards:
    movement_reward: 0.1          # Base reward (curriculum multipliers apply)
    intensity_penalty: 2.0        # Base penalty (curriculum multipliers apply)
    intensity_bonus: 0.1          # Base bonus (curriculum multipliers apply)
    substrate_reward: 0.2         # Base reward (curriculum multipliers apply)
  
  # Curriculum-specific reward bonuses
  curriculum_rewards:
    # Stage 1: Navigation bonuses
    navigation_bonuses:
      rightward_movement_bonus: 1.0     # Extra bonus for moving right
      boundary_avoidance_bonus: 2.0     # Extra bonus for avoiding boundaries
      episode_survival_bonus: 0.5       # Bonus per step survived
      distance_milestone_bonus: 5.0     # Bonus for distance milestones
      
    # Stage 2: Node management bonuses  
    management_bonuses:
      successful_spawn_bonus: 2.0       # Bonus for successful node spawning
      efficient_deletion_bonus: 3.0     # Bonus for efficient node deletion
      node_diversity_bonus: 1.5         # Bonus for maintaining node diversity
      connectivity_maintenance_bonus: 1.0  # Bonus for good connectivity
      
    # Stage 3: Optimization bonuses
    optimization_bonuses:
      task_completion_bonus: 50.0       # Large bonus for completing task
      efficiency_bonus: 10.0            # Bonus for efficient performance
      constraint_satisfaction_bonus: 5.0  # Bonus for satisfying all constraints
      perfection_bonus: 25.0            # Bonus for perfect execution
  
  # Reward structure - Edge rewards
  edge_reward:
    rightward_bonus: 0.5          # INCREASED from 0.1 for better directional incentive
    leftward_penalty: 0.05        # REDUCED from 0.1 for gentler penalty
  
  # Reward structure - Spawn rewards
  spawn_rewards:
    spawn_success_reward: 1.0     # Reward for successful durotaxis spawning
    spawn_failure_penalty: 1.0    # Penalty for failed durotaxis spawning
  
  # Reward structure - Delete rewards
  delete_reward:
    proper_deletion: 2.0          # Reward for proper node deletion
    persistence_penalty: 2.0      # Penalty for inappropriate persistence
  
  # Reward structure - Position rewards
  position_rewards:
    boundary_bonus: 0.1           # Bonus for boundary/frontier nodes
    left_edge_penalty: 0.2        # Penalty for being near left edge
    edge_position_penalty: 0.1    # Penalty for being near top/bottom edges
  
  # Reward structure - Termination rewards
  termination_rewards:
    success_reward: 100.0         # Large reward for reaching rightmost location
    out_of_bounds_penalty: -15.0  # Reduced from -30.0 for gentler learning
    no_nodes_penalty: -15.0       # Reduced from -30.0 for gentler learning
    leftward_drift_penalty: -15.0 # Reduced from -30.0 for gentler learning
    timeout_penalty: -5.0         # Reduced from -10.0 for gentler learning
    critical_nodes_penalty: -12.5 # Reduced from -25.0 for gentler learning
    
  # NEW: Node survival rewards to encourage maintenance
  survival_rewards:
    node_maintenance_bonus: 0.1   # Bonus per step for maintaining nodes
    episode_length_bonus: 0.05    # Bonus multiplier for longer episodes
    connectivity_bonus: 0.2       # Bonus for maintaining connectivity
    multi_node_bonus: 0.15        # Extra bonus for maintaining multiple nodes

# ============================================================================
# GraphInputEncoder Configuration
# ============================================================================
encoder:
  out_dim: 64                   # Output embedding dimension per node
  num_layers: 4                 # Number of transformer layers

# ============================================================================
# HybridActorCritic Configuration
# ============================================================================
actor_critic:
  hidden_dim: 128               # Hidden dimension for actor-critic networks
  num_discrete_actions: 2       # Number of discrete actions (spawn/delete)
  continuous_dim: 4             # Dimension of continuous action space (gamma, alpha, noise, theta)
  dropout_rate: 0.1             # Dropout rate for regularization
  
  # Value function components (list of reward components to predict)
  value_components:
    - 'total_reward'
    - 'graph_reward'
    - 'spawn_reward'
    - 'delete_reward'
    - 'edge_reward'
    - 'total_node_reward'

# ============================================================================
# Training Algorithm Configuration
# ============================================================================
algorithm:
  # GAE (Generalized Advantage Estimation) parameters
  gamma: 0.99                   # Discount factor
  gae_lambda: 0.95              # GAE lambda parameter
  
  # PPO (Proximal Policy Optimization) parameters
  ppo_epochs: 1                 # Number of PPO epochs per update
  clip_epsilon: 0.2             # PPO clipping parameter
  value_loss_coeff: 1.0         # Increased from 0.5 for better value estimation
  
  # Optimization parameters
  max_grad_norm: 1.0            # Increased from 0.5 for less aggressive clipping

# ============================================================================
# Device and Performance Configuration
# ============================================================================
system:
  device: 'auto'                # 'cuda', 'cpu', or 'auto' for automatic detection
  num_workers: 1                # Number of parallel workers (if applicable)
  seed: null                    # Random seed (null for no seeding)

# ============================================================================
# Logging and Monitoring Configuration
# ============================================================================
logging:
  tensorboard: false            # Enable TensorBoard logging
  wandb: false                  # Enable Weights & Biases logging
  save_model_every: null        # Save model every N episodes (null for disabled)
  save_best_model: true         # Save best performing model
  verbose: true                 # Verbose logging

# ============================================================================
# Experimental Configuration
# ============================================================================
experimental:
  # Action masking
  use_action_masking: true      # Enable action masking for invalid actions
  
  # Adaptive component weighting
  adaptive_component_weights: true    # Enable adaptive component weight updates
  
  # Substrate variation
  substrate_variation: true     # Enable substrate parameter variation during training
  
  # NEW: Curriculum learning system - ENABLED FOR BETTER LEARNING
  curriculum_learning:
    enable_curriculum: true           # Enable curriculum learning
    phase_1_episodes: 60             # Easy phase: focus on node maintenance (reduced for experiment)
    phase_2_episodes: 120            # Medium phase: balanced learning (reduced for experiment)
    # Phase 3: remaining episodes with full complexity
    
    # Phase-specific configurations
    phase_1_config:
      max_critical_nodes: 30          # Reduced complexity
      init_num_nodes: 3               # Start with more nodes
      no_nodes_penalty_multiplier: 0.5  # Reduced harsh penalties
      
    phase_2_config:
      max_critical_nodes: 40          # Gradual increase
      init_num_nodes: 2               # Moderate start
      no_nodes_penalty_multiplier: 0.75 # Gradual penalty increase
      
    phase_3_config:
      max_critical_nodes: 50          # Full complexity
      init_num_nodes: 1               # Standard start
      no_nodes_penalty_multiplier: 1.0  # Full penalties
  
  # NEW: Enhanced success criteria
  success_criteria:
    enable_multiple_criteria: true    # Use multiple success definitions
    survival_success_steps: 10        # Maintain nodes for 10+ steps
    reward_success_threshold: -20     # Achieve reasonable reward
    growth_success_nodes: 2           # Maintain connectivity (2+ nodes)
    exploration_success_steps: 15     # Long episode regardless of outcome
