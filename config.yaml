# ============================================================================
# Durotaxis RL Configuration File
# ============================================================================
# Organized configuration for the refactored reward system
# Core components: Delete (Priority 1), Spawn (Priority 2), Distance (Priority 3)

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
environment:
  # Basic parameters
  substrate_size: [600, 400]
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology limits
  init_num_nodes: 60
  max_critical_nodes: 40  # Marking threshold: marks nodes when N > this value
  state_space_multiplier: 2.0  # Observation capacity: K = multiplier × max_critical_nodes
  threshold_critical_nodes: 400
  max_steps: 1000
  
  # Simulation parameters
  delta_time: 3
  delta_intensity: 2.5
  consecutive_left_moves_limit: 30
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # ============================================================================
  # REWARD MODES (Special Modes for Ablation Studies)
  # ============================================================================
  # Normal mode (all three disabled): Delete + Spawn + Distance
  # Special modes: Enable ONE or MORE for ablation studies
  simple_delete_only_mode: false
  simple_spawn_only_mode: false
  centroid_distance_only_mode: false
  include_termination_rewards: false
  
  # ============================================================================
  # REWARD WEIGHTS (Components: Delete, Distance, Termination)
  # ============================================================================
  # Environment-level weights applied when composing total reward
  # Makes priority explicit in the task reward, not just critic loss weights
  reward_weights:
    delete_weight: 1.0         
    distance_weight: 0.001       
    termination_weight: 0.01
  
  # ============================================================================
  # CORE REWARD COMPONENTS 
  # ============================================================================
  
  # Delete Reward (Growth-friendly design)
  # Encourages BOTH marking AND growth while preventing collapse
  # Key: RULE 3 penalty is VERY mild to allow growth exploration
  delete_reward:
    # RULE 1: Marked + deleted (strong positive - teaches mark→delete)
    proper_deletion: 0.60
    
    # RULE 2: Marked + persists (weak negative - persistence is common)
    persistence_penalty: 0.10
    
    # RULE 3: Unmarked + deleted (VERY SMALL - allows growth)
    improper_deletion_penalty: 0.05
    
    # RULE 4: Unmarked + persists (positive - stabilizes high N)
    persistence_reward: 0.08
    
    # Normalization offset (prevents collapse-causing dilution at high N)
    # delete_reward = raw_reward / (prev_num_nodes + normalization_offset)
    normalization_offset: 4
    
    # N-Scaling for RULE 1 (encourages marking at high N)
    # RULE 1 reward becomes: base + scale_coefficient × min(N, scale_cap)
    # This keeps deletion signals meaningful even at large N
    rule1_n_scaling:
      enabled: true
      scale_coefficient: 0.02    # Reward increase per node
      scale_cap: 100             # Maximum N for scaling (prevents explosion)
    
    # Growth reward (encourages exploration beyond max_critical_nodes)
    # Applied when topology grows (new_N > prev_N)
    growth_reward:
      enabled: true
      coefficient: 0.02          # Reward per new node
    
    # Homeostatic band reward (keeps N in optimal range)
    # Prevents runaway growth by creating feedback pressure
    homeostasis_reward:
      enabled: true
      k1: 0.01                   # Growth encouragement below N_min
      k2: 0.02                   # Small bonus in ideal range
      k3: 0.01                   # Discouragement above N_mid
      K: 1.0                     # Strong penalty at/above N_max
    
    # Stability reward (teaches high-N maintenance in sweet spot range)
    # Applied when N is in optimal range AND no deletions occurred
    stability_reward:
      enabled: true
      base_reward: 0.10          # Flat reward for maintaining stable topology
      min_nodes: 40              # Minimum N to trigger (max_critical_nodes)
      max_nodes: 250             # Maximum N to trigger (stops encouraging large N)
    
    # Adaptive penalty scaling (RULE 3 shrinks as N grows)
    # Encourages growth: improper deletion penalty minimal at high N
    adaptive_scaling:
      high_node_threshold: 12        # Above this N: penalty scaled to 25% (very soft)
      low_node_threshold: 6          # Below this N: penalty at 100% (strict)
      high_penalty_scale: 0.25       # Scale factor at high N (1.00 → 0.25)
      low_penalty_scale: 1.0         # Scale factor at low N (full penalty)
      # Linear interpolation: goes 1.00 → 0.25 as N: 6 → 12
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: false
      shaping_coeff: 0.1              
      phi_weight_pending_marked: 1.0
      phi_weight_safe_unmarked: 0.25
  
  # Distance Reward (Centroid movement toward goal)
  distance_mode:
    use_delta_distance: true
    distance_reward_scale: 5.0
    
    # Substrate-aware normalization (bounds distance reward to [-1, 1])
    substrate_aware_scaling: true      # Use substrate gradient for scaling
    use_tanh_normalization: true       # Use tanh (true) or softsign (false)
    target_delta_x: 2.5              # Expected good step size for tuning
    tanh_scale: 1.4722                # atanh(0.9) = (approx.) 1.4722 for sensitivity tuning
    gradient_cap: null                # Optional: cap gradient for exponential substrates (e.g., 10.0)
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: false
      shaping_coeff: 0.05             # Lowest among all components
      phi_distance_scale: 1.0         # Scale for distance-to-goal potential
  
  # Termination Reward (Priority 4: Sparse terminal signals)
  # Sparse binary rewards: +1 for success, -1 for failures
  # Applied only at episode termination
  termination_rewards:
    success_reward: 1.0                      # Node reaches rightmost area
    critical_nodes_penalty: -1.0             # Too many nodes (> threshold)
    no_nodes_penalty: -1.0                   # Graph collapses to 0 nodes
    out_of_bounds_penalty: -1.0              # Node escapes substrate bounds
    leftward_drift_penalty: -1.0             # Centroid drifts left consecutively
    timeout_penalty: 0.0                     # Max steps reached (neutral)
  
  # ============================================================================
  # OBSERVATION SELECTION (Deletion-Aware)
  # ============================================================================
  # Prioritizes nodes important for deletion decisions:
  # - Rightward nodes (x-coordinate): spatial progress
  # - High-intensity nodes: candidates for marking (low intensity = marked)
  # - Representationally important nodes: high embedding norm
  observation_selection:
    method: topk_x
    w_x: 0.6           # Rightward bias (durotaxis direction)
    w_intensity: 0.3   # Intensity-based selection (marking criterion)
    w_norm: 0.1        # Representational importance

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
encoder:
  out_dim: 128
  num_layers: 4

actor_critic:
  hidden_dim: 128
  continuous_dim: 5
  dropout_rate: 0.2
  pretrained_weights: 'imagenet'
  
  # Backbone configuration
  backbone:
    input_adapter: 1ch_conv
    freeze_mode: all
    backbone_lr: 1.0e-4
    head_lr: 3.0e-4
  
  # Action parameter bounds
  action_parameter_bounds:
    delete_ratio: [0.0, 0.7]
    gamma: [0.5, 2.0]
    alpha: [0.5, 2.0]
    noise: [0.05, 0.15]
    theta: [-0.5236, 0.5236]  # -30 to 30 deg
  
  # Value function components
  value_components:
    - 'total_reward'
    - 'delete_reward'
    - 'distance_reward'
    - 'termination_reward'
  
  # Simplicial Embedding
  simplicial_embedding:
    enabled: true
    num_groups: 16
    temperature: 1.0

# ============================================================================
# TRAINING ALGORITHM
# ============================================================================
algorithm:
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  clip_epsilon: 0.2
  value_loss_coeff: 0.25
  max_grad_norm: 0.5
  target_kl: 0.03

# ============================================================================
# TRAINER CONFIGURATION
# ============================================================================
trainer:
  substrate_type: random
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.0001
  save_dir: "./training_results"
  
  # Resume training
  resume_training:
    enabled: false
    checkpoint_path: ""
    resume_from_best: false
    reset_optimizer: true
    reset_episode_count: true
  
  # Model selection
  model_selection:
    primary_metric: success_rate
    primary_weight: 1000.0
    secondary_metric: progress
    secondary_weight: 100.0
    tertiary_metric: return_mean
    tertiary_weight: 1.0
    window_episodes: 50
    min_improvement: 0.01
    min_episode_for_best_save: 20
  
  # Learning rate schedule
  warmup_updates_for_best: 5
  lr_warmup_updates: 200
  lr_total_updates: 10000
  
  # Batch training
  rollout_collection_mode: "steps"
  rollout_steps: 2048
  rollout_batch_size: 10
  update_epochs: 4
  minibatch_size: 64
  
  # Value clipping
  enable_value_clipping: true
  value_clip_epsilon: 0.2
  use_relative_value_clip: true 
  
  # Return normalization (prevents value loss explosion)
  normalize_returns: true
  return_scale: 10.0
  
  # Logging
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null
  
  # Detailed logging
  detailed_logging:
    enable_detailed_logging: true
    save_detailed_logs_to_json: true
    single_file_mode: true
    log_node_positions: true
    log_spawn_parameters: true
    log_persistent_ids: true
    log_connectivity: false
    log_substrate_values: false
    compress_logs: false
    max_log_file_size_mb: 50
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]           # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]            # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]     # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]       # Base value range for exponential substrate
  
  # Optimization
  entropy_bonus_coeff: 0.1
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Component weights (for multi-head value function)
  # Normalized to sum to 1.0 for balanced critic head learning
  component_weights:
    total_reward: 0.4
    delete_reward: 0.4
    distance_reward: 0.1
    termination_reward: 0.1
  
  # Policy loss weights
  policy_loss_weights:
    continuous_weight: 1.0
    entropy_weight: 0.01
  
  # Gradient scaling
  gradient_scaling:
    enable_adaptive_scaling: true
    gradient_norm_target: 1.0
    scaling_momentum: 0.9
    min_scaling_factor: 0.1
    max_scaling_factor: 10.0
    warmup_steps: 100
  
  # Adaptive scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
  
  # Reward normalization
  reward_normalization:
    enable_per_episode_norm: true
    enable_cross_episode_scaling: true
    min_episode_length: 3
    normalization_method: 'zscore'
  
  # Advantage weighting
  advantage_weighting:
    enable_learnable_weights: true
    enable_attention_weighting: true
    weight_learning_rate: 0.01
    weight_regularization: 0.001
  
  # Entropy regularization
  entropy_regularization:
    enable_adaptive_entropy: true
    entropy_coeff_start: 0.5
    entropy_coeff_end: 0.1
    entropy_decay_episodes: 500
    continuous_entropy_weight: 1.0
    min_entropy_threshold: 0.5

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================
system:
  device: 'auto'
  num_workers: 1
  seed: null

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  tensorboard: false
  wandb: false
  save_model_every: null
  save_best_model: true
  verbose: true
