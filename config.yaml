# Configuration file for Durotaxis Reinforcement Learning Training
# This file contains default parameters for all necessary classes used in DurotaxisTrainer

# ============================================================================
# DurotaxisTrainer Configuration
# ============================================================================
trainer:
  # Training parameters
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.0001  # Reduced from 0.0003 for more stable value learning
  save_dir: "./training_results"
  
  # Loss weighting and optimization
  entropy_bonus_coeff: 0.01
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Batch training configuration
  rollout_batch_size: 10      # Number of episodes to collect before updating
  update_epochs: 4            # Number of update epochs per batch
  minibatch_size: 64          # Size of minibatches during updates
  
  # Value clipping configuration
  enable_value_clipping: true # Enable PPO-style value clipping for stable training
  value_clip_epsilon: 0.2     # Clipping range for value predictions (0.1-0.3 typical)
  
  # Logging and checkpointing
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null  # Set to integer for periodic checkpoints, null for disabled
  
  # Environment setup parameters
  substrate_type: 'random'  # 'linear', 'exponential', or 'random'
  
  # Empty graph handling configuration
  empty_graph_handling:
    enable_graceful_recovery: true  # Enable graceful handling instead of breaking episodes
    recovery_num_nodes: 1           # Number of nodes to spawn when recovering from empty graph
    log_recoveries: true            # Log when empty graph recovery occurs
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]    # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]     # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]  # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]    # Base value range for exponential substrate
  
  # Component weights for multi-objective learning (adaptive)
  component_weights:
    total_reward: 1.0
    graph_reward: 0.4
    spawn_reward: 0.3
    delete_reward: 0.2
    edge_reward: 0.2
    total_node_reward: 0.3
  
  # Hybrid policy loss weights with gradient scaling
  policy_loss_weights:
    discrete_weight: 0.7     # Weight for discrete actions (spawn/delete)
    continuous_weight: 0.3   # Weight for continuous parameters
    entropy_weight: 0.01     # Exploration bonus weight
  
  # Adaptive gradient scaling for hybrid action spaces
  gradient_scaling:
    enable_adaptive_scaling: true    # Enable intelligent gradient balancing
    gradient_norm_target: 1.0        # Target gradient norm for balancing
    scaling_momentum: 0.9            # EMA momentum for gradient norm tracking
    min_scaling_factor: 0.1          # Minimum scaling to prevent collapse
    max_scaling_factor: 10.0         # Maximum scaling to prevent explosion
    warmup_steps: 100                # Steps to establish baseline gradient norms
  
  # Adaptive reward scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
    
  # Reward normalization system (two-tier approach)
  reward_normalization:
    enable_per_episode_norm: true     # Tier 1: Per-episode normalization
    enable_cross_episode_scaling: true  # Tier 2: Cross-episode adaptive scaling
    min_episode_length: 5             # Minimum episode length for normalization
    normalization_method: 'adaptive'  # 'zscore', 'minmax', or 'adaptive'
    
  # Enhanced learnable advantage weighting system
  advantage_weighting:
    enable_learnable_weights: true    # Enable learnable component weights
    enable_attention_weighting: true  # Enable attention-based dynamic weighting
    weight_learning_rate: 0.01        # Learning rate for component weights
    weight_regularization: 0.001      # L2 regularization for weight stability
    
  # Enhanced entropy regularization system
  entropy_regularization:
    enable_adaptive_entropy: true     # Enable adaptive entropy scheduling
    entropy_coeff_start: 0.3          # Increased from 0.1 for more exploration
    entropy_coeff_end: 0.01           # Increased from 0.001 to maintain exploration
    entropy_decay_episodes: 800       # Increased from 500 for slower decay
    discrete_entropy_weight: 1.5      # Increased from 1.0 for more discrete exploration
    continuous_entropy_weight: 1.0    # Increased from 0.5 for more continuous exploration
    min_entropy_threshold: 0.05       # Reduced from 0.1 to allow more focused policies

# ============================================================================
# DurotaxisEnv Configuration
# ============================================================================
environment:
  # Basic environment parameters
  substrate_size: [200, 200]
  substrate_type: 'linear'
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology parameters
  init_num_nodes: 1
  max_critical_nodes: 50
  threshold_critical_nodes: 200
  
  # Simulation parameters
  delta_time: 3
  delta_intensity: 2.50
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # Reward structure - Graph rewards
  graph_rewards:
    connectivity_penalty: 5.0     # Reduced from 10.0 for gentler learning
    growth_penalty: 5.0           # Reduced from 10.0 for gentler learning
    survival_reward: 0.1          # Increased from 0.01 for better maintenance incentive
    action_reward: 0.01           # Increased from 0.005 for better action incentive
  
  # Reward structure - Node rewards
  node_rewards:
    movement_reward: 0.01         # Reward multiplier for rightward movement
    intensity_penalty: 5.0        # Penalty for below-average substrate intensity
    intensity_bonus: 0.01         # Bonus for above-average substrate intensity
    substrate_reward: 0.05        # Reward multiplier for substrate intensity
  
  # Reward structure - Edge rewards
  edge_reward:
    rightward_bonus: 0.1          # Bonus for rightward edges
    leftward_penalty: 0.1         # Penalty for leftward edges
  
  # Reward structure - Spawn rewards
  spawn_rewards:
    spawn_success_reward: 1.0     # Reward for successful durotaxis spawning
    spawn_failure_penalty: 1.0    # Penalty for failed durotaxis spawning
  
  # Reward structure - Delete rewards
  delete_reward:
    proper_deletion: 2.0          # Reward for proper node deletion
    persistence_penalty: 2.0      # Penalty for inappropriate persistence
  
  # Reward structure - Position rewards
  position_rewards:
    boundary_bonus: 0.1           # Bonus for boundary/frontier nodes
    left_edge_penalty: 0.2        # Penalty for being near left edge
    edge_position_penalty: 0.1    # Penalty for being near top/bottom edges
  
  # Reward structure - Termination rewards
  termination_rewards:
    success_reward: 100.0         # Large reward for reaching rightmost location
    out_of_bounds_penalty: -15.0  # Reduced from -30.0 for gentler learning
    no_nodes_penalty: -15.0       # Reduced from -30.0 for gentler learning
    leftward_drift_penalty: -15.0 # Reduced from -30.0 for gentler learning
    timeout_penalty: -5.0         # Reduced from -10.0 for gentler learning
    critical_nodes_penalty: -12.5 # Reduced from -25.0 for gentler learning
    
  # NEW: Node survival rewards to encourage maintenance
  survival_rewards:
    node_maintenance_bonus: 0.1   # Bonus per step for maintaining nodes
    episode_length_bonus: 0.05    # Bonus multiplier for longer episodes
    connectivity_bonus: 0.2       # Bonus for maintaining connectivity
    multi_node_bonus: 0.15        # Extra bonus for maintaining multiple nodes

# ============================================================================
# GraphInputEncoder Configuration
# ============================================================================
encoder:
  out_dim: 64                   # Output embedding dimension per node
  num_layers: 4                 # Number of transformer layers

# ============================================================================
# HybridActorCritic Configuration
# ============================================================================
actor_critic:
  hidden_dim: 128               # Hidden dimension for actor-critic networks
  num_discrete_actions: 2       # Number of discrete actions (spawn/delete)
  continuous_dim: 4             # Dimension of continuous action space (gamma, alpha, noise, theta)
  dropout_rate: 0.1             # Dropout rate for regularization
  
  # Value function components (list of reward components to predict)
  value_components:
    - 'total_reward'
    - 'graph_reward'
    - 'spawn_reward'
    - 'delete_reward'
    - 'edge_reward'
    - 'total_node_reward'

# ============================================================================
# Training Algorithm Configuration
# ============================================================================
algorithm:
  # GAE (Generalized Advantage Estimation) parameters
  gamma: 0.99                   # Discount factor
  gae_lambda: 0.95              # GAE lambda parameter
  
  # PPO (Proximal Policy Optimization) parameters
  ppo_epochs: 1                 # Number of PPO epochs per update
  clip_epsilon: 0.2             # PPO clipping parameter
  value_loss_coeff: 1.0         # Increased from 0.5 for better value estimation
  
  # Optimization parameters
  max_grad_norm: 1.0            # Increased from 0.5 for less aggressive clipping

# ============================================================================
# Device and Performance Configuration
# ============================================================================
system:
  device: 'auto'                # 'cuda', 'cpu', or 'auto' for automatic detection
  num_workers: 1                # Number of parallel workers (if applicable)
  seed: null                    # Random seed (null for no seeding)

# ============================================================================
# Logging and Monitoring Configuration
# ============================================================================
logging:
  tensorboard: false            # Enable TensorBoard logging
  wandb: false                  # Enable Weights & Biases logging
  save_model_every: null        # Save model every N episodes (null for disabled)
  save_best_model: true         # Save best performing model
  verbose: true                 # Verbose logging

# ============================================================================
# Experimental Configuration
# ============================================================================
experimental:
  # Action masking
  use_action_masking: true      # Enable action masking for invalid actions
  
  # Adaptive component weighting
  adaptive_component_weights: true    # Enable adaptive component weight updates
  
  # Substrate variation
  substrate_variation: true     # Enable substrate parameter variation during training
  
  # NEW: Curriculum learning system
  curriculum_learning:
    enable_curriculum: true           # Enable curriculum learning
    phase_1_episodes: 300            # Easy phase: focus on node maintenance
    phase_2_episodes: 600            # Medium phase: balanced learning
    # Phase 3: remaining episodes with full complexity
    
    # Phase-specific configurations
    phase_1_config:
      max_critical_nodes: 30          # Reduced complexity
      init_num_nodes: 3               # Start with more nodes
      no_nodes_penalty_multiplier: 0.5  # Reduced harsh penalties
      
    phase_2_config:
      max_critical_nodes: 40          # Gradual increase
      init_num_nodes: 2               # Moderate start
      no_nodes_penalty_multiplier: 0.75 # Gradual penalty increase
      
    phase_3_config:
      max_critical_nodes: 50          # Full complexity
      init_num_nodes: 1               # Standard start
      no_nodes_penalty_multiplier: 1.0  # Full penalties
  
  # NEW: Enhanced success criteria
  success_criteria:
    enable_multiple_criteria: true    # Use multiple success definitions
    survival_success_steps: 10        # Maintain nodes for 10+ steps
    reward_success_threshold: -20     # Achieve reasonable reward
    growth_success_nodes: 2           # Maintain connectivity (2+ nodes)
    exploration_success_steps: 15     # Long episode regardless of outcome
