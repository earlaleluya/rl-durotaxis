# ============================================================================
# Durotaxis RL Configuration File
# ============================================================================
# Organized configuration for the refactored reward system
# Core components: Delete (Priority 1), Spawn (Priority 2), Distance (Priority 3)

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
environment:
  # Basic parameters
  substrate_size: [600, 400]
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology limits
  init_num_nodes: 5
  max_critical_nodes: 50
  threshold_critical_nodes: 200
  max_steps: 1000
  
  # Simulation parameters
  delta_time: 3
  delta_intensity: 2.50
  consecutive_left_moves_limit: 30
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # ============================================================================
  # REWARD MODES (Special Modes for Ablation Studies)
  # ============================================================================
  # Normal mode (all three disabled): Delete + Spawn + Distance
  # Special modes: Enable ONE or MORE for ablation studies
  simple_delete_only_mode: false
  simple_spawn_only_mode: false
  centroid_distance_only_mode: false
  include_termination_rewards: false
  
  # ============================================================================
  # REWARD WEIGHTS (Priority: Delete > Spawn > Distance)
  # ============================================================================
  # Environment-level weights applied when composing total reward
  # Makes priority explicit in the task reward, not just critic loss weights
  reward_weights:
    delete_weight: 1.0      # Priority 1: Full weight for deletion compliance
    spawn_weight: 0.75      # Priority 2: Reduced weight for spawning behavior
    distance_weight: 0.5    # Priority 3: Lower weight for distance signal
  
  # ============================================================================
  # CORE REWARD COMPONENTS (Refactored System)
  # ============================================================================
  # Priority: Delete > Spawn > Distance
  
  # Delete Reward (Priority 1: Proper deletion compliance)
  delete_reward:
    proper_deletion: 2.0
    persistence_penalty: 2.0
    improper_deletion_penalty: 2.0
    
    # PBRS: Potential-Based Reward Shaping (optional)
    # Higher shaping_coeff reinforces priority when enabled
    pbrs:
      enabled: true
      shaping_coeff: 0.1              # Higher than spawn (0.1 vs 0.05)
      phi_weight_pending_marked: 1.0
      phi_weight_safe_unmarked: 0.25
  
  # Spawn Reward (Priority 2: Intensity-based spawning)
  # Uses same magnitude for both simple and normal modes
  spawn_rewards:
    spawn_success_reward: 2.0     
    spawn_failure_penalty: 2.0     
    
    # PBRS: Potential-Based Reward Shaping (optional)
    # Lower shaping_coeff than delete reinforces priority when enabled
    pbrs:
      enabled: true
      shaping_coeff: 0.05             # Lower than delete (0.05 vs 0.1)
      phi_weight_spawnable: 1.0
  
  # Distance Reward (Priority 3: Centroid movement toward goal)
  distance_mode:
    use_delta_distance: true
    distance_reward_scale: 5.0
    
    # Termination reward handling
    terminal_reward_scale: 0.02
    clip_terminal_rewards: true
    terminal_reward_clip_value: 10.0

  
  # ============================================================================
  # TERMINATION REWARDS
  # ============================================================================
  termination_rewards:
    success_reward: 500.0
    out_of_bounds_penalty: -100.0
    no_nodes_penalty: -100.0
    leftward_drift_penalty: -50.0
    timeout_penalty: 0.0
    critical_nodes_penalty: -15.0
  
  # ============================================================================
  # OBSERVATION SELECTION
  # ============================================================================
  observation_selection:
    method: topk_x
    w_x: 1.0
    w_intensity: 0.0
    w_norm: 0.0

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
encoder:
  out_dim: 128
  num_layers: 4

actor_critic:
  hidden_dim: 128
  continuous_dim: 5
  dropout_rate: 0.2
  pretrained_weights: 'imagenet'
  
  # Backbone configuration
  backbone:
    input_adapter: 1ch_conv
    freeze_mode: all
    backbone_lr: 1.0e-4
    head_lr: 3.0e-4
  
  # Action parameter bounds
  action_parameter_bounds:
    delete_ratio: [0.0, 0.7]
    gamma: [0.5, 15.0]
    alpha: [0.5, 4.0]
    noise: [0.05, 0.5]
    theta: [-0.5236, 0.5236]  # -30 to 30 deg
  
  # Value function components
  value_components:
    - 'total_reward'
    - 'graph_reward'
    - 'spawn_reward'
    - 'delete_reward'
    - 'distance_signal'
  
  # Simplicial Embedding
  simplicial_embedding:
    enabled: true
    num_groups: 16
    temperature: 1.0

# ============================================================================
# TRAINING ALGORITHM
# ============================================================================
algorithm:
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  clip_epsilon: 0.1
  value_loss_coeff: 0.5
  max_grad_norm: 0.5

# ============================================================================
# TRAINER CONFIGURATION
# ============================================================================
trainer:
  substrate_type: linear
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.000001
  save_dir: "./training_results"
  
  # Resume training
  resume_training:
    enabled: false
    checkpoint_path: ""
    resume_from_best: false
    reset_optimizer: true
    reset_episode_count: true
  
  # Model selection
  model_selection:
    primary_metric: success_rate
    primary_weight: 1000.0
    secondary_metric: progress
    secondary_weight: 100.0
    tertiary_metric: return_mean
    tertiary_weight: 1.0
    window_episodes: 50
    min_improvement: 0.01
    min_episode_for_best_save: 50
  
  # Learning rate schedule
  warmup_updates_for_best: 5
  lr_warmup_updates: 200
  lr_total_updates: 10000
  
  # Batch training
  rollout_collection_mode: "steps"
  rollout_steps: 2048
  rollout_batch_size: 10
  update_epochs: 4
  minibatch_size: 64
  
  # Value clipping
  enable_value_clipping: true
  value_clip_epsilon: 0.2
  use_relative_value_clip: true 
  
  # Return normalization (prevents value loss explosion)
  normalize_returns: true
  return_scale: 10.0
  
  # Logging
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null
  
  # Detailed logging
  detailed_logging:
    enable_detailed_logging: true
    save_detailed_logs_to_json: true
    single_file_mode: true
    log_node_positions: true
    log_spawn_parameters: true
    log_persistent_ids: true
    log_connectivity: false
    log_substrate_values: false
    compress_logs: false
    max_log_file_size_mb: 50
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]           # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]            # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]     # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]       # Base value range for exponential substrate
  
  # Optimization
  entropy_bonus_coeff: 0.03
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Component weights (for multi-head value function)
  component_weights:
    total_reward: 1.0
    graph_reward: 0.4
    spawn_reward: 0.3
    delete_reward: 0.3
    distance_signal: 0.3
  
  # Policy loss weights
  policy_loss_weights:
    continuous_weight: 1.0
    entropy_weight: 0.01
  
  # Gradient scaling
  gradient_scaling:
    enable_adaptive_scaling: true
    gradient_norm_target: 1.0
    scaling_momentum: 0.9
    min_scaling_factor: 0.1
    max_scaling_factor: 10.0
    warmup_steps: 100
  
  # Adaptive scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
  
  # Reward normalization
  reward_normalization:
    enable_per_episode_norm: true
    enable_cross_episode_scaling: true
    min_episode_length: 3
    normalization_method: 'zscore'
  
  # Advantage weighting
  advantage_weighting:
    enable_learnable_weights: true
    enable_attention_weighting: true
    weight_learning_rate: 0.01
    weight_regularization: 0.001
  
  # Entropy regularization
  entropy_regularization:
    enable_adaptive_entropy: true
    entropy_coeff_start: 0.25
    entropy_coeff_end: 0.05
    entropy_decay_episodes: 300
    continuous_entropy_weight: 1.0
    min_entropy_threshold: 0.1

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================
system:
  device: 'auto'
  num_workers: 1
  seed: null

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  tensorboard: false
  wandb: false
  save_model_every: null
  save_best_model: true
  verbose: true

# ============================================================================
# EXPERIMENTAL FEATURES
# ============================================================================
experimental:
  # Multiple success criteria for episode evaluation
  # Used in train.py to determine if episode is successful beyond just reaching goal
  success_criteria:
    enable_multiple_criteria: true
    survival_success_steps: 10        # Episode length threshold for survival success
    reward_success_threshold: -20     # Total reward threshold for reward-based success
    growth_success_nodes: 2           # Node count threshold for growth success
    exploration_success_steps: 15     # Episode length threshold for exploration success
