# ============================================================================
# Durotaxis RL Configuration File
# ============================================================================
# Organized configuration for the refactored reward system
# Core components: Delete (Priority 1), Spawn (Priority 2), Distance (Priority 3)

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
environment:
  # Basic parameters
  substrate_size: [600, 400]
  substrate_params:
    m: 0.01
    b: 1.0
  
  # Node and topology limits
  init_num_nodes: 5
  max_critical_nodes: 50
  threshold_critical_nodes: 200
  max_steps: 1000
  
  # Simulation parameters
  delta_time: 3
  delta_intensity: 2.50
  consecutive_left_moves_limit: 30
  
  # Visualization
  flush_delay: 0.0001
  enable_visualization: false
  
  # ============================================================================
  # REWARD MODES (Special Modes for Ablation Studies)
  # ============================================================================
  # Normal mode (all three disabled): Delete + Spawn + Distance
  # Special modes: Enable ONE or MORE for ablation studies
  simple_delete_only_mode: false
  simple_spawn_only_mode: false
  centroid_distance_only_mode: false
  include_termination_rewards: false
  
  # ============================================================================
  # REWARD WEIGHTS (Components: Delete, Distance, Termination)
  # ============================================================================
  # Environment-level weights applied when composing total reward
  # Makes priority explicit in the task reward, not just critic loss weights
  reward_weights:
    delete_weight: 1.0         
    distance_weight: 1.0       
    termination_weight: 1.0   
  
  # ============================================================================
  # CORE REWARD COMPONENTS 
  # ============================================================================
  
  # Delete Reward (Priority 1: Proper deletion compliance)
  # Scaled to [-1, 1] by dividing by num_nodes
  # Use proper_deletion=1.0 for clean [-1, 1] range
  delete_reward:
    proper_deletion: 1.0
    persistence_penalty: 1.0
    improper_deletion_penalty: 1.0
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: true
      shaping_coeff: 0.1              
      phi_weight_pending_marked: 1.0
      phi_weight_safe_unmarked: 0.25
  
  # Spawn Reward (Priority 2: Intensity-based spawning)
  spawn_rewards:
    spawn_success_reward: 1.0     
    spawn_failure_penalty: 1.0     
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: true
      shaping_coeff: 0.05             # Lower than delete (0.05 vs 0.1)
      phi_weight_spawnable: 1.0
  
  # Distance Reward (Priority 3: Centroid movement toward goal)
  distance_mode:
    use_delta_distance: true
    distance_reward_scale: 5.0
    
    # Substrate-aware normalization (bounds distance reward to [-1, 1])
    substrate_aware_scaling: true      # Use substrate gradient for scaling
    use_tanh_normalization: true       # Use tanh (true) or softsign (false)
    target_delta_x: 0.05              # Expected good step size for tuning
    tanh_scale: 1.4722                # atanh(0.9) â‰ˆ 1.4722 for sensitivity tuning
    gradient_cap: null                # Optional: cap gradient for exponential substrates (e.g., 10.0)
    
    # PBRS: Potential-Based Reward Shaping (optional)
    pbrs:
      enabled: false
      shaping_coeff: 0.05             # Lowest among all components
      phi_distance_scale: 1.0         # Scale for distance-to-goal potential
  
  # Termination Reward (Priority 4: Sparse terminal signals)
  # Sparse binary rewards: +1 for success, -1 for failures
  # Applied only at episode termination
  termination_rewards:
    success_reward: 1.0                      # Node reaches rightmost area
    critical_nodes_penalty: -1.0             # Too many nodes (> threshold)
    no_nodes_penalty: -1.0                   # Graph collapses to 0 nodes
    out_of_bounds_penalty: -1.0              # Node escapes substrate bounds
    leftward_drift_penalty: -1.0             # Centroid drifts left consecutively
    timeout_penalty: 0.0                     # Max steps reached (neutral)
  
  # ============================================================================
  # OBSERVATION SELECTION
  # ============================================================================
  observation_selection:
    method: topk_x
    w_x: 1.0
    w_intensity: 0.0
    w_norm: 0.0

# ============================================================================
# NETWORK ARCHITECTURE
# ============================================================================
encoder:
  out_dim: 128
  num_layers: 4

actor_critic:
  hidden_dim: 128
  continuous_dim: 5
  dropout_rate: 0.2
  pretrained_weights: 'imagenet'
  
  # Backbone configuration
  backbone:
    input_adapter: 1ch_conv
    freeze_mode: all
    backbone_lr: 1.0e-4
    head_lr: 3.0e-4
  
  # Action parameter bounds
  action_parameter_bounds:
    delete_ratio: [0.0, 0.7]
    gamma: [0.5, 15.0]
    alpha: [0.5, 4.0]
    noise: [0.05, 0.5]
    theta: [-0.5236, 0.5236]  # -30 to 30 deg
  
  # Value function components
  value_components:
    - 'total_reward'
    - 'delete_reward'
    - 'distance_reward'
    - 'termination_reward'
  
  # Simplicial Embedding
  simplicial_embedding:
    enabled: true
    num_groups: 16
    temperature: 1.0

# ============================================================================
# TRAINING ALGORITHM
# ============================================================================
algorithm:
  gamma: 0.99
  gae_lambda: 0.95
  ppo_epochs: 4
  clip_epsilon: 0.1
  value_loss_coeff: 0.5
  max_grad_norm: 0.5

# ============================================================================
# TRAINER CONFIGURATION
# ============================================================================
trainer:
  substrate_type: linear
  total_episodes: 1000
  max_steps: 1000
  learning_rate: 0.000001
  save_dir: "./training_results"
  
  # Resume training
  resume_training:
    enabled: false
    checkpoint_path: ""
    resume_from_best: false
    reset_optimizer: true
    reset_episode_count: true
  
  # Model selection
  model_selection:
    primary_metric: success_rate
    primary_weight: 1000.0
    secondary_metric: progress
    secondary_weight: 100.0
    tertiary_metric: return_mean
    tertiary_weight: 1.0
    window_episodes: 50
    min_improvement: 0.01
    min_episode_for_best_save: 50
  
  # Learning rate schedule
  warmup_updates_for_best: 5
  lr_warmup_updates: 200
  lr_total_updates: 10000
  
  # Batch training
  rollout_collection_mode: "steps"
  rollout_steps: 2048
  rollout_batch_size: 10
  update_epochs: 4
  minibatch_size: 64
  
  # Value clipping
  enable_value_clipping: true
  value_clip_epsilon: 0.2
  use_relative_value_clip: true 
  
  # Return normalization (prevents value loss explosion)
  normalize_returns: true
  return_scale: 10.0
  
  # Logging
  moving_avg_window: 20
  log_every: 50
  progress_print_every: 5
  checkpoint_every: null
  
  # Detailed logging
  detailed_logging:
    enable_detailed_logging: true
    save_detailed_logs_to_json: true
    single_file_mode: true
    log_node_positions: true
    log_spawn_parameters: true
    log_persistent_ids: true
    log_connectivity: false
    log_substrate_values: false
    compress_logs: false
    max_log_file_size_mb: 50
  
  # Encoder configuration
  encoder_hidden_dim: 128
  encoder_output_dim: 64
  encoder_num_layers: 4
  
  # Random substrate parameter ranges (only used when substrate_type='random')
  random_substrate:
    linear_m_range: [0.01, 0.1]           # Slope range for linear substrate
    linear_b_range: [0.5, 2.0]            # Intercept range for linear substrate
    exponential_m_range: [0.01, 0.05]     # Growth rate range for exponential substrate
    exponential_b_range: [0.5, 1.5]       # Base value range for exponential substrate
  
  # Optimization
  entropy_bonus_coeff: 0.03
  weight_momentum: 0.9
  normalize_weights_every: 10
  
  # Component weights (for multi-head value function)
  # Normalized to sum to 1.0 for balanced critic head learning
  component_weights:
    total_reward: 0.25
    delete_reward: 0.25
    distance_reward: 0.25
    termination_reward: 0.25
  
  # Policy loss weights
  policy_loss_weights:
    continuous_weight: 1.0
    entropy_weight: 0.01
  
  # Gradient scaling
  gradient_scaling:
    enable_adaptive_scaling: true
    gradient_norm_target: 1.0
    scaling_momentum: 0.9
    min_scaling_factor: 0.1
    max_scaling_factor: 10.0
    warmup_steps: 100
  
  # Adaptive scaling
  adaptive_scaling:
    enable_adaptive_scaling: true
    scaling_warmup_episodes: 50
  
  # Reward normalization
  reward_normalization:
    enable_per_episode_norm: true
    enable_cross_episode_scaling: true
    min_episode_length: 3
    normalization_method: 'zscore'
  
  # Advantage weighting
  advantage_weighting:
    enable_learnable_weights: true
    enable_attention_weighting: true
    weight_learning_rate: 0.01
    weight_regularization: 0.001
  
  # Entropy regularization
  entropy_regularization:
    enable_adaptive_entropy: true
    entropy_coeff_start: 0.25
    entropy_coeff_end: 0.05
    entropy_decay_episodes: 300
    continuous_entropy_weight: 1.0
    min_entropy_threshold: 0.1

# ============================================================================
# SYSTEM CONFIGURATION
# ============================================================================
system:
  device: 'auto'
  num_workers: 1
  seed: null

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  tensorboard: false
  wandb: false
  save_model_every: null
  save_best_model: true
  verbose: true
