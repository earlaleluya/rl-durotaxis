// RL-Durotaxis System Architecture Flowchart (3-Component + PBRS System)
// Visualizes the 4-head multi-head critic with PBRS optional shaping
// To generate image: dot -Tpng rl_flowchart_color.dot -o rl_flowchart.png

digraph RLDurotaxis {
    rankdir=TB;
    bgcolor="white";
    node [shape=box, style="rounded,filled", fontname="Arial", fontsize=10];
    edge [fontname="Arial", fontsize=9];
    
    // ===== CONFIG LAYER =====
    subgraph cluster_config {
        label="Configuration (config.yaml)";
        color="#7a7a7a";
        style="rounded,filled";
        fillcolor="#e8ebf2";
        
        config [label=<
            <b>Reward Weights</b> (Environment):<br/>
            • delete: 1.0 (Priority 1)<br/>
            • distance: 1.0 (Priority 2)<br/>
            • termination: 1.0 (Priority 3)<br/>
            <br/>
            <b>Critic Weights</b> (Training):<br/>
            • All normalized to 0.25 each<br/>
            (total, delete, distance, termination)
        >, fillcolor="white"];
    }
    
    // ===== ENVIRONMENT LAYER =====
    subgraph cluster_environment {
        label="Environment (durotaxis_env.py)";
        color="#5a5a5a";
        style="rounded,filled";
        fillcolor="#ffe6e6";
        
        state [label=<
            <b>Graph State s<sub>t</sub></b><br/>
            • Node features [N, 11]<br/>
            • Edge features [E, 3]<br/>
            • Graph features [19]
        >, fillcolor="white"];
        
        action_apply [label=<
            <b>Apply Actions</b><br/>
            1. Delete leftmost (ratio)<br/>
            2. Update graph state<br/>
            3. Update physics
        >, fillcolor="#ffcccc"];
        
        reward_comp [label=<
            <b>Compute 3 Core Rewards + PBRS</b><br/>
            <br/>
            <font color="darkred"><b>Delete</b></font> (Priority 1): proper/improper/persistence<br/>
            <font color="purple">PBRS</font>: φ(pending/marked) → [-1, 1], coeff=0.1<br/>
            <br/>
            <font color="darkblue"><b>Distance</b></font> (Priority 2): centroid movement<br/>
            <font color="purple">PBRS</font>: φ(distance-to-goal) → [-1, 1], coeff=0.05<br/>
            <br/>
            <font color="darkorange"><b>Termination</b></font> (Priority 3): sparse binary<br/>
            Success +1, Failures -1, Timeout 0
        >, fillcolor="#ffb3b3"];
        
        reward_weight [label=<
            <b>Apply Env Weights &amp; PBRS</b><br/>
            <br/>
            For each component c:<br/>
            r<sub>c</sub> = base<sub>c</sub> + F(s,a,s')<br/>
            F = γ·φ(s') - φ(s)<br/>
            <br/>
            total = w<sub>del</sub>·r<sub>del</sub> + w<sub>dist</sub>·r<sub>dist</sub> + w<sub>term</sub>·r<sub>term</sub><br/>
            <br/>
            <b>Output: 4 components</b><br/>
            {total, delete, distance, termination}
        >, fillcolor="#ff9999"];
        
        next_state [label=<
            <b>Transition</b><br/>
            s<sub>t+1</sub>, r<sub>t</sub> [4], done, info
        >, fillcolor="white"];
    }
    
    // ===== NETWORK LAYER =====
    subgraph cluster_network {
        label="Neural Network (actor_critic.py + encoder.py)";
        color="#5a5a5a";
        style="rounded,filled";
        fillcolor="#e6f3ff";
        
        encoder [label=<
            <b>Graph Encoder</b><br/>
            <br/>
            • Feature projection<br/>
            • 4-layer TransformerConv<br/>
            • Edge-aware attention<br/>
            <br/>
            → embeddings [N+1, 128]
        >, fillcolor="white"];
        
        actor [label=<
            <b>Actor Network (Policy π)</b><br/>
            <br/>
            • ResNet18 backbone<br/>
            • MLP head<br/>
            <br/>
            <b>Output:</b><br/>
            • μ [N, 5], log_σ [N, 5]<br/>
            <br/>
            <b>Actions:</b><br/>
            [delete_ratio, γ, α, noise, θ]
        >, fillcolor="#cce5ff"];
        
        critic [label=<
            <b>Critic Network (4 Heads)</b><br/>
            <br/>
            • ResNet18 backbone<br/>
            • 4 independent MLP heads<br/>
            <br/>
            <b>Output V(s):</b><br/>
            • V<sub>total</sub>: Combined value<br/>
            • V<sub>delete</sub>: Delete-specific value<br/>
            • V<sub>distance</sub>: Distance-specific value<br/>
            • V<sub>termination</sub>: Termination-specific value
        >, fillcolor="#99ccff"];
    }
    
    // ===== PPO UPDATE LAYER =====
    subgraph cluster_ppo {
        label="PPO Training (train.py)";
        color="#5a5a5a";
        style="rounded,filled";
        fillcolor="#e6ffe6";
        
        trajectory [label=<
            <b>Trajectory Buffer</b><br/>
            <br/>
            Collect transitions:<br/>
            (s, a, r [4], V [4], log_π, done)
        >, fillcolor="white"];
        
        gae [label=<
            <b>Per-Component GAE</b><br/>
            <br/>
            <u>For EACH component c separately:</u><br/>
            1. Extract r<sub>t</sub><sup>c</sup> from reward dict<br/>
            2. Extract V<sup>c</sup>(s) from critic head c<br/>
            3. δ<sub>t</sub><sup>c</sup> = r<sub>t</sub><sup>c</sup> + γV<sup>c</sup>' - V<sup>c</sup><br/>
            4. A<sub>t</sub><sup>c</sup> = Σ (γλ)<sup>l</sup> · δ<sub>t+l</sub><sup>c</sup><br/>
            5. G<sub>t</sub><sup>c</sup> = A<sub>t</sub><sup>c</sup> + V<sup>c</sup><br/>
            6. Normalize A<sub>t</sub><sup>c</sup> per component<br/>
            <br/>
            → 4 independent {returns, advantages}<br/>
            <font color="darkgreen"><b>Benefit:</b> Component-specific learning!</font>
        >, fillcolor="#ccffcc"];
        
        policy_loss [label=<
            <b>Policy Loss</b><br/>
            <br/>
            <b>Weighted Advantage:</b><br/>
            A<sub>weighted</sub> = Σ w<sub>i</sub> · A<sub>i</sub><br/>
            (all 4 components)<br/>
            <br/>
            L<sub>policy</sub> = -mean(<br/>
              min(ratio·A<sub>weighted</sub>,<br/>
                  clip(ratio)·A<sub>weighted</sub>)<br/>
            )
        >, fillcolor="#99ff99"];
        
        value_loss [label=<
            <b>Value Loss (Component-Specific)</b><br/>
            <br/>
            <u>For each component c:</u><br/>
            L<sub>c</sub> = MSE(V<sub>c</sub>, G<sub>c</sub><sup>component</sup>)<br/>
            <br/>
            <font color="darkred">V<sub>delete</sub> → G<sub>delete</sub> (delete-only)</font><br/>
            <font color="darkblue">V<sub>distance</sub> → G<sub>distance</sub> (distance-only)</font><br/>
            <font color="darkorange">V<sub>termination</sub> → G<sub>termination</sub> (term-only)</font><br/>
            <br/>
            L<sub>value</sub> = Σ w<sub>c</sub> · L<sub>c</sub><br/>
            <br/>
            <b>Weights:</b><br/>
            All normalized to 0.25 each
        >, fillcolor="#66ff66"];
        
        total_loss [label=<
            <b>Total Loss</b><br/>
            <br/>
            L = L<sub>policy</sub> + L<sub>value</sub> - β·H(π)
        >, fillcolor="#33ff33"];
        
        update [label=<
            <b>Backpropagation</b><br/>
            <br/>
            Update actor &amp; critic<br/>
            parameters (θ)
        >, fillcolor="#00ff00"];
    }
    
    // ===== METRICS =====
    metrics [label=<
        <b>Logging &amp; Metrics</b><br/>
        <br/>
        • Episode rewards (4 comp.)<br/>
        • Loss components<br/>
        • Network stats<br/>
        • Checkpoint saves
    >, shape=note, fillcolor="#ffffcc"];
    
    // ===== FLOW CONNECTIONS =====
    
    // Config → Components
    config -> reward_weight [label="reward_weights", color=blue];
    config -> value_loss [label="component_weights", color=blue];
    
    // Environment flow
    state -> encoder [label="observation", color=black, penwidth=2];
    encoder -> actor [label="embeddings", color=black];
    encoder -> critic [label="embeddings", color=black];
    
    actor -> action_apply [label=<a<sub>t</sub> [N, 5]>, color=darkgreen, penwidth=2];
    action_apply -> reward_comp [label="state changes", color=black];
    reward_comp -> reward_weight [label="3 core + PBRS", color=orange];
    reward_weight -> next_state [label="4 components", color=red, penwidth=2];
    
    critic -> trajectory [label=<V(s) [4]>, color=purple];
    next_state -> trajectory [label=<r, s', done>, color=black];
    actor -> trajectory [label=<a, log_π>, color=darkgreen];
    
    // PPO flow
    trajectory -> gae [label="batch ready", color=black, penwidth=2];
    gae -> policy_loss [label="advantages [4]", color=purple];
    gae -> value_loss [label="returns [4]", color=purple];
    
    policy_loss -> total_loss [color=darkgreen];
    value_loss -> total_loss [color=darkred];
    total_loss -> update [label="gradient", color=black, penwidth=2];
    
    update -> actor [label="update θ", color=blue, style=dashed];
    update -> critic [label="update θ", color=blue, style=dashed];
    
    // Metrics
    trajectory -> metrics [style=dotted, color=gray];
    total_loss -> metrics [style=dotted, color=gray];
    
    // Loop back
    next_state -> state [label=<s ← s'>, color=gray, style=dashed];
    
    // ===== LEGEND =====
    subgraph cluster_legend {
        label="Key Points";
        color="#5a5a5a";
        style="rounded,filled";
        fillcolor="#fffaf0";
        
        leg1 [label=<
            <b>Priority Hierarchy:</b><br/>
            Delete (P1) &gt; Distance (P2) &gt; Termination (P3)<br/>
            Default equal weights (configurable)
        >, shape=note, fillcolor="lightyellow"];
        
        leg2 [label=<
            <b>PBRS (Optional):</b><br/>
            F(s,a,s') = γ·φ(s') - φ(s)<br/>
            Policy-invariant shaping<br/>
            Delete: φ(pending/marked)<br/>
            Distance: φ(distance-to-goal)
        >, shape=note, fillcolor="lightcyan"];
        
        leg3 [label=<
            <b>Policy Composition:</b><br/>
            Uses weighted sum of<br/>
            ALL 4 component advantages
        >, shape=note, fillcolor="lightgreen"];
        
        leg4 [label=<
            <b>Per-Component GAE:</b><br/>
            Each critic head learns<br/>
            component-specific values<br/>
            → Agent can identify &amp;<br/>
            improve weak components!
        >, shape=note, fillcolor="lightpink"];
    }
    
    // Position legend at bottom
    {rank=same; metrics; leg1; leg2; leg3; leg4;}
}
